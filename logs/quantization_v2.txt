/home/vimagupta123/shivam/Mixture-Compressor-MoE/mixture_compressor/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/vimagupta123/shivam/Mixture-Compressor-MoE/mixture_compressor/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/vimagupta123/shivam/Mixture-Compressor-MoE/mixture_compressor/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Arguments: Namespace(model='/home/vimagupta123/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/ffe1a706bacbd5abddc5ff99432ee38f7e0662fb', wbits='2bit', attn_bits='4bit', dataset='wikitext2', load_quantized=False, seed=0, nsamples=128, percdamp=0.01, groupsize=128, num_fewshot=0, batch_size=1, attn_implementation='eager', sym=False, act_order=False, multigpu=False, eval_ppl=True, tasks='', save=True, pack=True, use_flash_attention_2=False, r=7, mixed_type='mixed', h_experts=2, l_experts=2, precisions='./experts_mixture_bit_selection/experts_mixture_bitwidth_combination_20bit.pkl', saving_path='/home/vimagupta123/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1-2.5b')
Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/19 [00:00<00:14,  1.24it/s]Loading checkpoint shards:  11%|█         | 2/19 [00:01<00:13,  1.23it/s]Loading checkpoint shards:  16%|█▌        | 3/19 [00:02<00:13,  1.23it/s]Loading checkpoint shards:  21%|██        | 4/19 [00:03<00:11,  1.28it/s]Loading checkpoint shards:  26%|██▋       | 5/19 [00:03<00:10,  1.31it/s]Loading checkpoint shards:  32%|███▏      | 6/19 [00:04<00:09,  1.35it/s]Loading checkpoint shards:  37%|███▋      | 7/19 [00:05<00:08,  1.40it/s]Loading checkpoint shards:  42%|████▏     | 8/19 [00:05<00:07,  1.43it/s]Loading checkpoint shards:  47%|████▋     | 9/19 [00:07<00:09,  1.01it/s]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:08<00:08,  1.11it/s]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:09<00:06,  1.17it/s]Loading checkpoint shards:  63%|██████▎   | 12/19 [00:09<00:05,  1.21it/s]Loading checkpoint shards:  68%|██████▊   | 13/19 [00:21<00:24,  4.08s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [00:23<00:17,  3.48s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [00:24<00:11,  2.78s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [00:25<00:07,  2.35s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [00:27<00:04,  2.04s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [00:28<00:01,  1.86s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:29<00:00,  1.66s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:29<00:00,  1.57s/it]
Starting ...
Ready.
Quantizing layer 1/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 2421.341   | -          | -          | 4.696   |
torch.uint8
| self_attn.k_proj-4              | 776.290    | -          | -          | 4.231   |
torch.uint8
| self_attn.v_proj-4              | 37.983     | -          | -          | 4.200   |
torch.uint8
| self_attn.o_proj-4              | 1.639      | -          | -          | 4.482   |
torch.uint8
| block_sparse_moe.gate-4         | 9.231      | -          | -          | 3.714   |
torch.uint8
| block_sparse_moe.experts.0.w1-2 | 1949.655   | -          | -          | 3.826   |
torch.uint8
| block_sparse_moe.experts.0.w2-2 | 17.320     | -          | -          | 12.696  |
torch.uint8
| block_sparse_moe.experts.0.w3-2 | 1873.638   | -          | -          | 3.840   |
torch.uint8
| block_sparse_moe.experts.1.w1-3 | 543.319    | -          | -          | 4.243   |
torch.int32
| block_sparse_moe.experts.1.w2-3 | 6.453      | -          | -          | 13.585  |
torch.int32
| block_sparse_moe.experts.1.w3-3 | 512.241    | -          | -          | 4.195   |
torch.int32
| block_sparse_moe.experts.2.w1-3 | 891.582    | -          | -          | 4.275   |
torch.int32
| block_sparse_moe.experts.2.w2-3 | 9.781      | -          | -          | 13.503  |
torch.int32
| block_sparse_moe.experts.2.w3-3 | 829.341    | -          | -          | 4.311   |
torch.int32
| block_sparse_moe.experts.3.w1-3 | 692.350    | -          | -          | 4.279   |
torch.int32
| block_sparse_moe.experts.3.w2-3 | 6.720      | -          | -          | 13.545  |
torch.int32
| block_sparse_moe.experts.3.w3-3 | 645.497    | -          | -          | 4.259   |
torch.int32
| block_sparse_moe.experts.4.w1-2 | 3501.848   | -          | -          | 3.792   |
torch.uint8
| block_sparse_moe.experts.4.w2-2 | 34.733     | -          | -          | 12.635  |
torch.uint8
| block_sparse_moe.experts.4.w3-2 | 3323.990   | -          | -          | 3.779   |
torch.uint8
| block_sparse_moe.experts.5.w1-2 | 4823.415   | -          | -          | 3.813   |
torch.uint8
| block_sparse_moe.experts.5.w2-2 | 44.017     | -          | -          | 12.688  |
torch.uint8
| block_sparse_moe.experts.5.w3-2 | 4568.606   | -          | -          | 3.750   |
torch.uint8
| block_sparse_moe.experts.6.w1-3 | 241.919    | -          | -          | 4.244   |
torch.int32
| block_sparse_moe.experts.6.w2-3 | 15.497     | -          | -          | 13.593  |
torch.int32
| block_sparse_moe.experts.6.w3-3 | 228.271    | -          | -          | 4.259   |
torch.int32
| block_sparse_moe.experts.7.w1-2 | 3549.606   | -          | -          | 3.825   |
torch.uint8
| block_sparse_moe.experts.7.w2-2 | 32.005     | -          | -          | 12.587  |
torch.uint8
| block_sparse_moe.experts.7.w3-2 | 3385.143   | -          | -          | 3.818   |
torch.uint8
+--------------------------------+------------+------------+------------+---------+


Quantizing layer 2/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 1916.936   | -          | -          | 4.478   |
torch.uint8
| self_attn.k_proj-4              | 863.295    | -          | -          | 4.322   |
torch.uint8
| self_attn.v_proj-4              | 202.197    | -          | -          | 4.271   |
torch.uint8
| self_attn.o_proj-4              | 5.684      | -          | -          | 4.458   |
torch.uint8
| block_sparse_moe.gate-4         | 17.501     | -          | -          | 3.748   |
torch.uint8
| block_sparse_moe.experts.0.w1-3 | 1683.032   | -          | -          | 4.248   |
torch.int32
| block_sparse_moe.experts.0.w2-3 | 12.528     | -          | -          | 13.718  |
torch.int32
| block_sparse_moe.experts.0.w3-3 | 1602.894   | -          | -          | 4.217   |
torch.int32
| block_sparse_moe.experts.1.w1-2 | 12817.271  | -          | -          | 3.812   |
torch.uint8
| block_sparse_moe.experts.1.w2-2 | 99.062     | -          | -          | 12.787  |
torch.uint8
| block_sparse_moe.experts.1.w3-2 | 12033.650  | -          | -          | 3.801   |
torch.uint8
| block_sparse_moe.experts.2.w1-3 | 1605.499   | -          | -          | 4.259   |
torch.int32
| block_sparse_moe.experts.2.w2-3 | 12.658     | -          | -          | 13.885  |
torch.int32
| block_sparse_moe.experts.2.w3-3 | 1526.212   | -          | -          | 4.263   |
torch.int32
| block_sparse_moe.experts.3.w1-3 | 1474.167   | -          | -          | 4.233   |
torch.int32
| block_sparse_moe.experts.3.w2-3 | 11852.377  | -          | -          | 13.630  |
torch.int32
| block_sparse_moe.experts.3.w3-3 | 1452.796   | -          | -          | 4.308   |
torch.int32
| block_sparse_moe.experts.4.w1-3 | 2632.708   | -          | -          | 4.339   |
torch.int32
| block_sparse_moe.experts.4.w2-3 | 20.561     | -          | -          | 13.716  |
torch.int32
| block_sparse_moe.experts.4.w3-3 | 2484.241   | -          | -          | 4.249   |
torch.int32
| block_sparse_moe.experts.5.w1-3 | 2049.985   | -          | -          | 4.239   |
torch.int32
| block_sparse_moe.experts.5.w2-3 | 15.147     | -          | -          | 13.812  |
torch.int32
| block_sparse_moe.experts.5.w3-3 | 1939.413   | -          | -          | 4.281   |
torch.int32
| block_sparse_moe.experts.6.w1-2 | 7810.748   | -          | -          | 3.841   |
torch.uint8
| block_sparse_moe.experts.6.w2-2 | 72.210     | -          | -          | 12.671  |
torch.uint8
| block_sparse_moe.experts.6.w3-2 | 7453.977   | -          | -          | 3.855   |
torch.uint8
| block_sparse_moe.experts.7.w1-1 | 20846.283  | -          | -          | 1.757   |
torch.uint8
| block_sparse_moe.experts.7.w2-1 | 159.086    | -          | -          | 6.466   |
torch.uint8
| block_sparse_moe.experts.7.w3-1 | 19214.117  | -          | -          | 1.730   |
torch.uint8
+--------------------------------+------------+------------+------------+---------+


Quantizing layer 3/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 13608.543  | -          | -          | 4.439   |
torch.uint8
| self_attn.k_proj-4              | 4595.333   | -          | -          | 4.395   |
torch.uint8
| self_attn.v_proj-4              | 1108.368   | -          | -          | 4.385   |
torch.uint8
| self_attn.o_proj-4              | 5.080      | -          | -          | 4.456   |
torch.uint8
| block_sparse_moe.gate-4         | 22.739     | -          | -          | 3.723   |
torch.uint8
| block_sparse_moe.experts.0.w1-2 | 18618.439  | -          | -          | 3.762   |
torch.uint8
| block_sparse_moe.experts.0.w2-2 | 193.473    | -          | -          | 12.657  |
torch.uint8
| block_sparse_moe.experts.0.w3-2 | 17466.234  | -          | -          | 3.806   |
torch.uint8
| block_sparse_moe.experts.1.w1-3 | 4660.434   | -          | -          | 4.318   |
torch.int32
| block_sparse_moe.experts.1.w2-3 | 46.271     | -          | -          | 13.638  |
torch.int32
| block_sparse_moe.experts.1.w3-3 | 4321.710   | -          | -          | 4.295   |
torch.int32
| block_sparse_moe.experts.2.w1-3 | 4094.702   | -          | -          | 4.343   |
torch.int32
| block_sparse_moe.experts.2.w2-3 | 38.557     | -          | -          | 13.631  |
torch.int32
| block_sparse_moe.experts.2.w3-3 | 3835.307   | -          | -          | 4.236   |
torch.int32
| block_sparse_moe.experts.3.w1-3 | 3672.213   | -          | -          | 4.252   |
torch.int32
| block_sparse_moe.experts.3.w2-3 | 35.761     | -          | -          | 13.732  |
torch.int32
| block_sparse_moe.experts.3.w3-3 | 3461.863   | -          | -          | 4.279   |
torch.int32
| block_sparse_moe.experts.4.w1-3 | 3766.007   | -          | -          | 4.207   |
torch.int32
| block_sparse_moe.experts.4.w2-3 | 38.137     | -          | -          | 13.600  |
torch.int32
| block_sparse_moe.experts.4.w3-3 | 3550.555   | -          | -          | 4.260   |
torch.int32
| block_sparse_moe.experts.5.w1-2 | 20556.262  | -          | -          | 3.838   |
torch.uint8
| block_sparse_moe.experts.5.w2-2 | 192.669    | -          | -          | 12.626  |
torch.uint8
| block_sparse_moe.experts.5.w3-2 | 19334.395  | -          | -          | 3.811   |
torch.uint8
| block_sparse_moe.experts.6.w1-3 | 3310.803   | -          | -          | 4.239   |
torch.int32
| block_sparse_moe.experts.6.w2-3 | 33.728     | -          | -          | 13.790  |
torch.int32
| block_sparse_moe.experts.6.w3-3 | 3082.240   | -          | -          | 4.280   |
torch.int32
| block_sparse_moe.experts.7.w1-1 | 25496.953  | -          | -          | 1.716   |
torch.uint8
| block_sparse_moe.experts.7.w2-1 | 224.570    | -          | -          | 6.473   |
torch.uint8
| block_sparse_moe.experts.7.w3-1 | 24399.686  | -          | -          | 1.721   |
torch.uint8
+--------------------------------+------------+------------+------------+---------+


Quantizing layer 4/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 7063.768   | -          | -          | 4.466   |
torch.uint8
| self_attn.k_proj-4              | 2320.522   | -          | -          | 4.324   |
torch.uint8
| self_attn.v_proj-4              | 945.849    | -          | -          | 4.348   |
torch.uint8
| self_attn.o_proj-4              | 12.760     | -          | -          | 4.404   |
torch.uint8
| block_sparse_moe.gate-4         | 34.851     | -          | -          | 3.800   |
torch.uint8
| block_sparse_moe.experts.0.w1-3 | 5805.337   | -          | -          | 4.242   |
torch.int32
| block_sparse_moe.experts.0.w2-3 | 65.819     | -          | -          | 13.640  |
torch.int32
| block_sparse_moe.experts.0.w3-3 | 5455.255   | -          | -          | 4.195   |
torch.int32
| block_sparse_moe.experts.1.w1-2 | 35111.508  | -          | -          | 3.758   |
torch.uint8
| block_sparse_moe.experts.1.w2-2 | 412.029    | -          | -          | 12.691  |
torch.uint8
| block_sparse_moe.experts.1.w3-2 | 33183.008  | -          | -          | 3.808   |
torch.uint8
| block_sparse_moe.experts.2.w1-3 | 7137.216   | -          | -          | 4.279   |
torch.int32
| block_sparse_moe.experts.2.w2-3 | 85.392     | -          | -          | 13.726  |
torch.int32
| block_sparse_moe.experts.2.w3-3 | 6717.030   | -          | -          | 4.222   |
torch.int32
| block_sparse_moe.experts.3.w1-1 | 51988.988  | -          | -          | 1.736   |
torch.uint8
| block_sparse_moe.experts.3.w2-1 | 635.671    | -          | -          | 6.460   |
torch.uint8
| block_sparse_moe.experts.3.w3-1 | 48149.676  | -          | -          | 1.750   |
torch.uint8
| block_sparse_moe.experts.4.w1-3 | 6109.697   | -          | -          | 4.243   |
torch.int32
| block_sparse_moe.experts.4.w2-3 | 76.122     | -          | -          | 13.759  |
torch.int32
| block_sparse_moe.experts.4.w3-3 | 5747.424   | -          | -          | 4.245   |
torch.int32
| block_sparse_moe.experts.5.w1-2 | 38636.816  | -          | -          | 3.779   |
torch.uint8
| block_sparse_moe.experts.5.w2-2 | 479.213    | -          | -          | 12.703  |
torch.uint8
| block_sparse_moe.experts.5.w3-2 | 36133.887  | -          | -          | 3.758   |
torch.uint8
| block_sparse_moe.experts.6.w1-3 | 8057.777   | -          | -          | 4.207   |
torch.int32
| block_sparse_moe.experts.6.w2-3 | 100.484    | -          | -          | 13.802  |
torch.int32
| block_sparse_moe.experts.6.w3-3 | 7557.710   | -          | -          | 4.187   |
torch.int32
| block_sparse_moe.experts.7.w1-3 | 6718.271   | -          | -          | 4.270   |
torch.int32
| block_sparse_moe.experts.7.w2-3 | 82.815     | -          | -          | 13.677  |
torch.int32
| block_sparse_moe.experts.7.w3-3 | 6255.160   | -          | -          | 4.250   |
torch.int32
+--------------------------------+------------+------------+------------+---------+


Quantizing layer 5/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 10853.162  | -          | -          | 4.577   |
torch.uint8
| self_attn.k_proj-4              | 3338.442   | -          | -          | 4.362   |
torch.uint8
| self_attn.v_proj-4              | 1482.629   | -          | -          | 4.340   |
torch.uint8
| self_attn.o_proj-4              | 17.717     | -          | -          | 4.415   |
torch.uint8
| block_sparse_moe.gate-4         | 40.581     | -          | -          | 3.775   |
torch.uint8
| block_sparse_moe.experts.0.w1-3 | 10354.759  | -          | -          | 4.275   |
torch.int32
| block_sparse_moe.experts.0.w2-3 | 184.945    | -          | -          | 13.877  |
torch.int32
| block_sparse_moe.experts.0.w3-3 | 9424.393   | -          | -          | 4.235   |
torch.int32
| block_sparse_moe.experts.1.w1-1 | 105702.352 | -          | -          | 1.787   |
torch.uint8
| block_sparse_moe.experts.1.w2-1 | 1872.834   | -          | -          | 6.491   |
torch.uint8
| block_sparse_moe.experts.1.w3-1 | 93635.250  | -          | -          | 1.735   |
torch.uint8
| block_sparse_moe.experts.2.w1-3 | 10930.121  | -          | -          | 4.275   |
torch.int32
| block_sparse_moe.experts.2.w2-3 | 198.904    | -          | -          | 13.795  |
torch.int32
| block_sparse_moe.experts.2.w3-3 | 10094.154  | -          | -          | 4.280   |
torch.int32
| block_sparse_moe.experts.3.w1-3 | 9973.151   | -          | -          | 4.306   |
torch.int32
| block_sparse_moe.experts.3.w2-3 | 177.962    | -          | -          | 13.748  |
torch.int32
| block_sparse_moe.experts.3.w3-3 | 9245.327   | -          | -          | 4.248   |
torch.int32
| block_sparse_moe.experts.4.w1-2 | 53547.098  | -          | -          | 3.785   |
torch.uint8
| block_sparse_moe.experts.4.w2-2 | 1016.703   | -          | -          | 12.781  |
torch.uint8
| block_sparse_moe.experts.4.w3-2 | 48751.770  | -          | -          | 3.841   |
torch.uint8
| block_sparse_moe.experts.5.w1-2 | 50123.758  | -          | -          | 3.807   |
torch.uint8
| block_sparse_moe.experts.5.w2-2 | 877.338    | -          | -          | 12.820  |
torch.uint8
| block_sparse_moe.experts.5.w3-2 | 46028.430  | -          | -          | 3.875   |
torch.uint8
| block_sparse_moe.experts.6.w1-3 | 10900.242  | -          | -          | 4.320   |
torch.int32
| block_sparse_moe.experts.6.w2-3 | 184.803    | -          | -          | 13.715  |
torch.int32
| block_sparse_moe.experts.6.w3-3 | 10046.907  | -          | -          | 4.255   |
torch.int32
| block_sparse_moe.experts.7.w1-3 | 12178.766  | -          | -          | 4.314   |
torch.int32
| block_sparse_moe.experts.7.w2-3 | 206.160    | -          | -          | 13.807  |
torch.int32
| block_sparse_moe.experts.7.w3-3 | 11237.270  | -          | -          | 4.289   |
torch.int32
+--------------------------------+------------+------------+------------+---------+


Quantizing layer 6/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 13571.564  | -          | -          | 4.496   |
torch.uint8
| self_attn.k_proj-4              | 4146.308   | -          | -          | 4.375   |
torch.uint8
| self_attn.v_proj-4              | 1606.106   | -          | -          | 4.329   |
torch.uint8
| self_attn.o_proj-4              | 43.195     | -          | -          | 4.516   |
torch.uint8
| block_sparse_moe.gate-4         | 44.469     | -          | -          | 3.742   |
torch.uint8
| block_sparse_moe.experts.0.w1-2 | 85727.367  | -          | -          | 3.885   |
torch.uint8
| block_sparse_moe.experts.0.w2-2 | 1937.395   | -          | -          | 12.804  |
torch.uint8
| block_sparse_moe.experts.0.w3-2 | 76427.500  | -          | -          | 3.795   |
torch.uint8
| block_sparse_moe.experts.1.w1-3 | 17313.191  | -          | -          | 4.318   |
torch.int32
| block_sparse_moe.experts.1.w2-3 | 373.063    | -          | -          | 13.839  |
torch.int32
| block_sparse_moe.experts.1.w3-3 | 15296.657  | -          | -          | 4.247   |
torch.int32
| block_sparse_moe.experts.2.w1-1 | 182113.406 | -          | -          | 1.744   |
torch.uint8
| block_sparse_moe.experts.2.w2-1 | 3928.413   | -          | -          | 6.530   |
torch.uint8
| block_sparse_moe.experts.2.w3-1 | 157713.031 | -          | -          | 1.696   |
torch.uint8
| block_sparse_moe.experts.3.w1-3 | 13485.941  | -          | -          | 4.283   |
torch.int32
| block_sparse_moe.experts.3.w2-3 | 282.265    | -          | -          | 13.665  |
torch.int32
| block_sparse_moe.experts.3.w3-3 | 12041.060  | -          | -          | 4.259   |
torch.int32
| block_sparse_moe.experts.4.w1-2 | 81619.875  | -          | -          | 3.817   |
torch.uint8
| block_sparse_moe.experts.4.w2-2 | 1845.351   | -          | -          | 12.649  |
torch.uint8
| block_sparse_moe.experts.4.w3-2 | 72727.766  | -          | -          | 3.807   |
torch.uint8
| block_sparse_moe.experts.5.w1-3 | 17925.674  | -          | -          | 4.295   |
torch.int32
| block_sparse_moe.experts.5.w2-3 | 405.763    | -          | -          | 13.709  |
torch.int32
| block_sparse_moe.experts.5.w3-3 | 15907.536  | -          | -          | 4.305   |
torch.int32
| block_sparse_moe.experts.6.w1-3 | 14441.098  | -          | -          | 4.214   |
torch.int32
| block_sparse_moe.experts.6.w2-3 | 311.018    | -          | -          | 13.697  |
torch.int32
| block_sparse_moe.experts.6.w3-3 | 13004.621  | -          | -          | 4.350   |
torch.int32
| block_sparse_moe.experts.7.w1-3 | 13334.135  | -          | -          | 4.286   |
torch.int32
| block_sparse_moe.experts.7.w2-3 | 284.421    | -          | -          | 13.692  |
torch.int32
| block_sparse_moe.experts.7.w3-3 | 11892.816  | -          | -          | 4.297   |
torch.int32
+--------------------------------+------------+------------+------------+---------+


Quantizing layer 7/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 14051.302  | -          | -          | 4.467   |
torch.uint8
| self_attn.k_proj-4              | 4485.114   | -          | -          | 4.305   |
torch.uint8
| self_attn.v_proj-4              | 1887.242   | -          | -          | 4.366   |
torch.uint8
| self_attn.o_proj-4              | 62.663     | -          | -          | 4.521   |
torch.uint8
| block_sparse_moe.gate-4         | 42.696     | -          | -          | 3.754   |
torch.uint8
| block_sparse_moe.experts.0.w1-3 | 23340.951  | -          | -          | 4.233   |
torch.int32
| block_sparse_moe.experts.0.w2-3 | 559.031    | -          | -          | 13.722  |
torch.int32
| block_sparse_moe.experts.0.w3-3 | 20849.512  | -          | -          | 4.227   |
torch.int32
| block_sparse_moe.experts.1.w1-3 | 18253.250  | -          | -          | 4.258   |
torch.int32
| block_sparse_moe.experts.1.w2-3 | 470.792    | -          | -          | 13.680  |
torch.int32
| block_sparse_moe.experts.1.w3-3 | 16596.557  | -          | -          | 4.256   |
torch.int32
| block_sparse_moe.experts.2.w1-3 | 19866.098  | -          | -          | 4.325   |
torch.int32
| block_sparse_moe.experts.2.w2-3 | 501.577    | -          | -          | 13.775  |
torch.int32
| block_sparse_moe.experts.2.w3-3 | 17786.674  | -          | -          | 4.268   |
torch.int32
| block_sparse_moe.experts.3.w1-3 | 22872.572  | -          | -          | 4.231   |
torch.int32
| block_sparse_moe.experts.3.w2-3 | 565.777    | -          | -          | 13.641  |
torch.int32
| block_sparse_moe.experts.3.w3-3 | 20299.215  | -          | -          | 4.270   |
torch.int32
| block_sparse_moe.experts.4.w1-2 | 80625.469  | -          | -          | 3.759   |
torch.uint8
| block_sparse_moe.experts.4.w2-2 | 2117.578   | -          | -          | 12.816  |
torch.uint8
| block_sparse_moe.experts.4.w3-2 | 73938.703  | -          | -          | 3.767   |
torch.uint8
| block_sparse_moe.experts.5.w1-2 | 101143.625 | -          | -          | 3.833   |
torch.uint8
| block_sparse_moe.experts.5.w2-2 | 2695.610   | -          | -          | 12.670  |
torch.uint8
| block_sparse_moe.experts.5.w3-2 | 92226.781  | -          | -          | 3.774   |
torch.uint8
| block_sparse_moe.experts.6.w1-1 | 190800.625 | -          | -          | 1.739   |
torch.uint8
| block_sparse_moe.experts.6.w2-1 | 4642.366   | -          | -          | 6.552   |
torch.uint8
| block_sparse_moe.experts.6.w3-1 | 166400.734 | -          | -          | 1.711   |
torch.uint8
| block_sparse_moe.experts.7.w1-3 | 19684.818  | -          | -          | 4.252   |
torch.int32
| block_sparse_moe.experts.7.w2-3 | 525.964    | -          | -          | 13.720  |
torch.int32
| block_sparse_moe.experts.7.w3-3 | 17699.740  | -          | -          | 4.198   |
torch.int32
+--------------------------------+------------+------------+------------+---------+


Quantizing layer 8/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 20123.098  | -          | -          | 4.556   |
torch.uint8
| self_attn.k_proj-4              | 6181.249   | -          | -          | 4.387   |
torch.uint8
| self_attn.v_proj-4              | 3120.039   | -          | -          | 4.314   |
torch.uint8
| self_attn.o_proj-4              | 143.067    | -          | -          | 4.496   |
torch.uint8
| block_sparse_moe.gate-4         | 47.404     | -          | -          | 3.756   |
torch.uint8
| block_sparse_moe.experts.0.w1-3 | 23692.832  | -          | -          | 4.349   |
torch.int32
| block_sparse_moe.experts.0.w2-3 | 719.385    | -          | -          | 13.701  |
torch.int32
| block_sparse_moe.experts.0.w3-3 | 21208.504  | -          | -          | 4.263   |
torch.int32
| block_sparse_moe.experts.1.w1-3 | 24884.260  | -          | -          | 4.217   |
torch.int32
| block_sparse_moe.experts.1.w2-3 | 718.568    | -          | -          | 13.653  |
torch.int32
| block_sparse_moe.experts.1.w3-3 | 22356.734  | -          | -          | 4.307   |
torch.int32
| block_sparse_moe.experts.2.w1-2 | 136108.844 | -          | -          | 3.826   |
torch.uint8
| block_sparse_moe.experts.2.w2-2 | 4131.794   | -          | -          | 12.634  |
torch.uint8
| block_sparse_moe.experts.2.w3-2 | 122858.234 | -          | -          | 3.787   |
torch.uint8
| block_sparse_moe.experts.3.w1-1 | 242478.594 | -          | -          | 1.698   |
torch.uint8
| block_sparse_moe.experts.3.w2-1 | 7344.421   | -          | -          | 6.498   |
torch.uint8
| block_sparse_moe.experts.3.w3-1 | 209450.734 | -          | -          | 1.749   |
torch.uint8
| block_sparse_moe.experts.4.w1-3 | 22308.074  | -          | -          | 4.291   |
torch.int32
| block_sparse_moe.experts.4.w2-3 | 666.776    | -          | -          | 13.689  |
torch.int32
| block_sparse_moe.experts.4.w3-3 | 20068.672  | -          | -          | 4.220   |
torch.int32
| block_sparse_moe.experts.5.w1-3 | 23400.801  | -          | -          | 4.209   |
torch.int32
| block_sparse_moe.experts.5.w2-3 | 634.299    | -          | -          | 13.635  |
torch.int32
| block_sparse_moe.experts.5.w3-3 | 20927.832  | -          | -          | 4.238   |
torch.int32
| block_sparse_moe.experts.6.w1-3 | 20784.969  | -          | -          | 4.239   |
torch.int32
| block_sparse_moe.experts.6.w2-3 | 572.001    | -          | -          | 13.804  |
torch.int32
| block_sparse_moe.experts.6.w3-3 | 18660.453  | -          | -          | 4.273   |
torch.int32
| block_sparse_moe.experts.7.w1-2 | 171164.047 | -          | -          | 3.860   |
torch.uint8
| block_sparse_moe.experts.7.w2-2 | 5213.005   | -          | -          | 12.735  |
torch.uint8
| block_sparse_moe.experts.7.w3-2 | 151077.641 | -          | -          | 3.756   |
torch.uint8
+--------------------------------+------------+------------+------------+---------+


Quantizing layer 9/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 15951.937  | -          | -          | 4.434   |
torch.uint8
| self_attn.k_proj-4              | 4677.644   | -          | -          | 4.321   |
torch.uint8
| self_attn.v_proj-4              | 2870.092   | -          | -          | 4.394   |
torch.uint8
| self_attn.o_proj-4              | 173.583    | -          | -          | 4.455   |
torch.uint8
| block_sparse_moe.gate-4         | 52.578     | -          | -          | 3.783   |
torch.uint8
| block_sparse_moe.experts.0.w1-3 | 25807.535  | -          | -          | 4.215   |
torch.int32
| block_sparse_moe.experts.0.w2-3 | 801.662    | -          | -          | 13.659  |
torch.int32
| block_sparse_moe.experts.0.w3-3 | 23907.348  | -          | -          | 4.267   |
torch.int32
| block_sparse_moe.experts.1.w1-3 | 31570.588  | -          | -          | 4.315   |
torch.int32
| block_sparse_moe.experts.1.w2-3 | 995.461    | -          | -          | 13.811  |
torch.int32
| block_sparse_moe.experts.1.w3-3 | 28517.963  | -          | -          | 4.229   |
torch.int32
| block_sparse_moe.experts.2.w1-1 | 265014.875 | -          | -          | 1.738   |
torch.uint8
| block_sparse_moe.experts.2.w2-1 | 7860.252   | -          | -          | 6.581   |
torch.uint8
| block_sparse_moe.experts.2.w3-1 | 228130.156 | -          | -          | 1.736   |
torch.uint8
| block_sparse_moe.experts.3.w1-2 | 157217.531 | -          | -          | 3.828   |
torch.uint8
| block_sparse_moe.experts.3.w2-2 | 5040.396   | -          | -          | 12.718  |
torch.uint8
| block_sparse_moe.experts.3.w3-2 | 141695.328 | -          | -          | 3.855   |
torch.uint8
| block_sparse_moe.experts.4.w1-3 | 26522.896  | -          | -          | 4.212   |
torch.int32
| block_sparse_moe.experts.4.w2-3 | 818.684    | -          | -          | 13.548  |
torch.int32
| block_sparse_moe.experts.4.w3-3 | 24325.096  | -          | -          | 4.236   |
torch.int32
| block_sparse_moe.experts.5.w1-3 | 28314.076  | -          | -          | 4.246   |
torch.int32
| block_sparse_moe.experts.5.w2-3 | 884.082    | -          | -          | 13.636  |
torch.int32
| block_sparse_moe.experts.5.w3-3 | 25761.461  | -          | -          | 4.228   |
torch.int32
| block_sparse_moe.experts.6.w1-3 | 30326.742  | -          | -          | 4.223   |
torch.int32
| block_sparse_moe.experts.6.w2-3 | 943.439    | -          | -          | 13.617  |
torch.int32
| block_sparse_moe.experts.6.w3-3 | 27844.201  | -          | -          | 4.200   |
torch.int32
| block_sparse_moe.experts.7.w1-2 | 156969.062 | -          | -          | 3.755   |
torch.uint8
| block_sparse_moe.experts.7.w2-2 | 5365.475   | -          | -          | 12.685  |
torch.uint8
| block_sparse_moe.experts.7.w3-2 | 145967.094 | -          | -          | 3.760   |
torch.uint8
+--------------------------------+------------+------------+------------+---------+


Quantizing layer 10/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 22347.141  | -          | -          | 4.553   |
torch.uint8
| self_attn.k_proj-4              | 6713.561   | -          | -          | 4.307   |
torch.uint8
| self_attn.v_proj-4              | 3527.731   | -          | -          | 4.262   |
torch.uint8
| self_attn.o_proj-4              | 207.357    | -          | -          | 4.413   |
torch.uint8
| block_sparse_moe.gate-4         | 52.344     | -          | -          | 3.776   |
torch.uint8
| block_sparse_moe.experts.0.w1-3 | 33972.820  | -          | -          | 4.285   |
torch.int32
| block_sparse_moe.experts.0.w2-3 | 1277.746   | -          | -          | 13.556  |
torch.int32
| block_sparse_moe.experts.0.w3-3 | 32234.207  | -          | -          | 4.238   |
torch.int32
| block_sparse_moe.experts.1.w1-3 | 31130.387  | -          | -          | 4.222   |
torch.int32
| block_sparse_moe.experts.1.w2-3 | 1175.109   | -          | -          | 13.584  |
torch.int32
| block_sparse_moe.experts.1.w3-3 | 29666.852  | -          | -          | 4.236   |
torch.int32
| block_sparse_moe.experts.2.w1-3 | 31571.410  | -          | -          | 4.214   |
torch.int32
| block_sparse_moe.experts.2.w2-3 | 1107.295   | -          | -          | 13.715  |
torch.int32
| block_sparse_moe.experts.2.w3-3 | 29771.070  | -          | -          | 4.239   |
torch.int32
| block_sparse_moe.experts.3.w1-2 | 202642.062 | -          | -          | 3.841   |
torch.uint8
| block_sparse_moe.experts.3.w2-2 | 7928.185   | -          | -          | 12.741  |
torch.uint8
| block_sparse_moe.experts.3.w3-2 | 191431.562 | -          | -          | 3.769   |
torch.uint8
| block_sparse_moe.experts.4.w1-1 | 272059.438 | -          | -          | 1.699   |
torch.uint8
| block_sparse_moe.experts.4.w2-1 | 10495.510  | -          | -          | 6.525   |
torch.uint8
| block_sparse_moe.experts.4.w3-1 | 252025.938 | -          | -          | 1.765   |
torch.uint8
| block_sparse_moe.experts.5.w1-3 | 30345.691  | -          | -          | 4.266   |
torch.int32
| block_sparse_moe.experts.5.w2-3 | 991.398    | -          | -          | 13.666  |
torch.int32
| block_sparse_moe.experts.5.w3-3 | 28430.141  | -          | -          | 4.235   |
torch.int32
| block_sparse_moe.experts.6.w1-2 | 163651.969 | -          | -          | 3.807   |
torch.uint8
| block_sparse_moe.experts.6.w2-2 | 5557.521   | -          | -          | 12.634  |
torch.uint8
| block_sparse_moe.experts.6.w3-2 | 152678.125 | -          | -          | 3.840   |
torch.uint8
| block_sparse_moe.experts.7.w1-3 | 28101.039  | -          | -          | 4.255   |
torch.int32
| block_sparse_moe.experts.7.w2-3 | 1024.453   | -          | -          | 13.618  |
torch.int32
| block_sparse_moe.experts.7.w3-3 | 26658.637  | -          | -          | 4.222   |
torch.int32
+--------------------------------+------------+------------+------------+---------+


Quantizing layer 11/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 20564.926  | -          | -          | 4.431   |
torch.uint8
| self_attn.k_proj-4              | 6379.896   | -          | -          | 4.323   |
torch.uint8
| self_attn.v_proj-4              | 3289.305   | -          | -          | 4.285   |
torch.uint8
| self_attn.o_proj-4              | 311.699    | -          | -          | 4.413   |
torch.uint8
| block_sparse_moe.gate-4         | 36.376     | -          | -          | 3.774   |
torch.uint8
| block_sparse_moe.experts.0.w1-3 | 28904.057  | -          | -          | 4.204   |
torch.int32
| block_sparse_moe.experts.0.w2-3 | 1238.391   | -          | -          | 13.578  |
torch.int32
| block_sparse_moe.experts.0.w3-3 | 28472.486  | -          | -          | 4.207   |
torch.int32
| block_sparse_moe.experts.1.w1-3 | 38429.879  | -          | -          | 4.157   |
torch.int32
| block_sparse_moe.experts.1.w2-3 | 1878.593   | -          | -          | 13.543  |
torch.int32
| block_sparse_moe.experts.1.w3-3 | 38985.133  | -          | -          | 4.171   |
torch.int32
| block_sparse_moe.experts.2.w1-2 | 192425.375 | -          | -          | 3.775   |
torch.uint8
| block_sparse_moe.experts.2.w2-2 | 8458.074   | -          | -          | 12.592  |
torch.uint8
| block_sparse_moe.experts.2.w3-2 | 184359.219 | -          | -          | 3.758   |
torch.uint8
| block_sparse_moe.experts.3.w1-3 | 35271.773  | -          | -          | 4.250   |
torch.int32
| block_sparse_moe.experts.3.w2-3 | 1635.525   | -          | -          | 13.573  |
torch.int32
| block_sparse_moe.experts.3.w3-3 | 35288.812  | -          | -          | 4.225   |
torch.int32
| block_sparse_moe.experts.4.w1-3 | 29237.508  | -          | -          | 4.162   |
torch.int32
| block_sparse_moe.experts.4.w2-3 | 1184.564   | -          | -          | 13.697  |
torch.int32
| block_sparse_moe.experts.4.w3-3 | 28550.139  | -          | -          | 4.204   |
torch.int32
| block_sparse_moe.experts.5.w1-3 | 34029.066  | -          | -          | 4.173   |
torch.int32
| block_sparse_moe.experts.5.w2-3 | 1478.399   | -          | -          | 13.580  |
torch.int32
| block_sparse_moe.experts.5.w3-3 | 33524.203  | -          | -          | 4.163   |
torch.int32
| block_sparse_moe.experts.6.w1-1 | 207155.219 | -          | -          | 1.712   |
torch.uint8
| block_sparse_moe.experts.6.w2-1 | 8451.508   | -          | -          | 6.412   |
torch.uint8
| block_sparse_moe.experts.6.w3-1 | 194581.625 | -          | -          | 1.711   |
torch.uint8
| block_sparse_moe.experts.7.w1-2 | 213470.938 | -          | -          | 3.760   |
torch.uint8
| block_sparse_moe.experts.7.w2-2 | 8780.156   | -          | -          | 12.560  |
torch.uint8
| block_sparse_moe.experts.7.w3-2 | 206880.125 | -          | -          | 3.838   |
torch.uint8
+--------------------------------+------------+------------+------------+---------+


Quantizing layer 12/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 24459.516  | -          | -          | 4.550   |
torch.uint8
| self_attn.k_proj-4              | 7343.062   | -          | -          | 4.370   |
torch.uint8
| self_attn.v_proj-4              | 5088.166   | -          | -          | 4.509   |
torch.uint8
| self_attn.o_proj-4              | 498.777    | -          | -          | 4.559   |
torch.uint8
| block_sparse_moe.gate-4         | 37.138     | -          | -          | 3.800   |
torch.uint8
| block_sparse_moe.experts.0.w1-3 | 36280.703  | -          | -          | 4.327   |
torch.int32
| block_sparse_moe.experts.0.w2-3 | 1540.642   | -          | -          | 13.938  |
torch.int32
| block_sparse_moe.experts.0.w3-3 | 35738.797  | -          | -          | 4.284   |
torch.int32
| block_sparse_moe.experts.1.w1-3 | 39707.293  | -          | -          | 4.408   |
torch.int32
| block_sparse_moe.experts.1.w2-3 | 1800.768   | -          | -          | 13.979  |
torch.int32
| block_sparse_moe.experts.1.w3-3 | 39579.051  | -          | -          | 4.306   |
torch.int32
| block_sparse_moe.experts.2.w1-3 | 38453.797  | -          | -          | 4.352   |
torch.int32
| block_sparse_moe.experts.2.w2-3 | 1869.164   | -          | -          | 13.944  |
torch.int32
| block_sparse_moe.experts.2.w3-3 | 38614.395  | -          | -          | 4.333   |
torch.int32
| block_sparse_moe.experts.3.w1-1 | 116829.570 | -          | -          | 1.747   |
torch.uint8
| block_sparse_moe.experts.3.w2-1 | 6766.006   | -          | -          | 6.528   |
torch.uint8
| block_sparse_moe.experts.3.w3-1 | 118646.617 | -          | -          | 1.750   |
torch.uint8
| block_sparse_moe.experts.4.w1-2 | 188497.109 | -          | -          | 3.852   |
torch.uint8
| block_sparse_moe.experts.4.w2-2 | 9216.165   | -          | -          | 12.859  |
torch.uint8
| block_sparse_moe.experts.4.w3-2 | 193538.531 | -          | -          | 3.871   |
torch.uint8
| block_sparse_moe.experts.5.w1-3 | 38525.664  | -          | -          | 4.277   |
torch.int32
| block_sparse_moe.experts.5.w2-3 | 1975.670   | -          | -          | 13.843  |
torch.int32
| block_sparse_moe.experts.5.w3-3 | 40354.332  | -          | -          | 4.304   |
torch.int32
| block_sparse_moe.experts.6.w1-3 | 42646.297  | -          | -          | 4.334   |
torch.int32
| block_sparse_moe.experts.6.w2-3 | 1927.307   | -          | -          | 13.811  |
torch.int32
| block_sparse_moe.experts.6.w3-3 | 42251.301  | -          | -          | 4.341   |
torch.int32
| block_sparse_moe.experts.7.w1-2 | 217784.812 | -          | -          | 3.928   |
torch.uint8
| block_sparse_moe.experts.7.w2-2 | 9772.479   | -          | -          | 12.869  |
torch.uint8
| block_sparse_moe.experts.7.w3-2 | 218081.641 | -          | -          | 3.878   |
torch.uint8
+--------------------------------+------------+------------+------------+---------+


Quantizing layer 13/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 34178.996  | -          | -          | 4.566   |
torch.uint8
| self_attn.k_proj-4              | 9924.682   | -          | -          | 4.416   |
torch.uint8
| self_attn.v_proj-4              | 6270.092   | -          | -          | 4.432   |
torch.uint8
| self_attn.o_proj-4              | 672.726    | -          | -          | 4.495   |
torch.uint8
| block_sparse_moe.gate-4         | 48.008     | -          | -          | 3.796   |
torch.uint8
| block_sparse_moe.experts.0.w1-3 | 43550.750  | -          | -          | 4.288   |
torch.int32
| block_sparse_moe.experts.0.w2-3 | 2461.379   | -          | -          | 13.784  |
torch.int32
| block_sparse_moe.experts.0.w3-3 | 45298.148  | -          | -          | 4.334   |
torch.int32
| block_sparse_moe.experts.1.w1-2 | 274455.812 | -          | -          | 3.851   |
torch.uint8
| block_sparse_moe.experts.1.w2-2 | 13897.656  | -          | -          | 12.854  |
torch.uint8
| block_sparse_moe.experts.1.w3-2 | 279211.875 | -          | -          | 3.881   |
torch.uint8
| block_sparse_moe.experts.2.w1-3 | 45890.840  | -          | -          | 4.282   |
torch.int32
| block_sparse_moe.experts.2.w2-3 | 2225.734   | -          | -          | 13.865  |
torch.int32
| block_sparse_moe.experts.2.w3-3 | 47350.699  | -          | -          | 4.323   |
torch.int32
| block_sparse_moe.experts.3.w1-3 | 39165.250  | -          | -          | 4.345   |
torch.int32
| block_sparse_moe.experts.3.w2-3 | 1774.610   | -          | -          | 13.846  |
torch.int32
| block_sparse_moe.experts.3.w3-3 | 40618.289  | -          | -          | 4.337   |
torch.int32
| block_sparse_moe.experts.4.w1-3 | 40788.859  | -          | -          | 4.278   |
torch.int32
| block_sparse_moe.experts.4.w2-3 | 2115.825   | -          | -          | 13.882  |
torch.int32
| block_sparse_moe.experts.4.w3-3 | 42311.258  | -          | -          | 4.370   |
torch.int32
| block_sparse_moe.experts.5.w1-2 | 271079.688 | -          | -          | 3.898   |
torch.uint8
| block_sparse_moe.experts.5.w2-2 | 12764.424  | -          | -          | 12.923  |
torch.uint8
| block_sparse_moe.experts.5.w3-2 | 277870.938 | -          | -          | 3.886   |
torch.uint8
| block_sparse_moe.experts.6.w1-3 | 34314.762  | -          | -          | 4.303   |
torch.int32
| block_sparse_moe.experts.6.w2-3 | 1611.585   | -          | -          | 13.861  |
torch.int32
| block_sparse_moe.experts.6.w3-3 | 36213.742  | -          | -          | 4.336   |
torch.int32
| block_sparse_moe.experts.7.w1-1 | 93627.062  | -          | -          | 1.736   |
torch.uint8
| block_sparse_moe.experts.7.w2-1 | 7409.133   | -          | -          | 6.549   |
torch.uint8
| block_sparse_moe.experts.7.w3-1 | 96855.367  | -          | -          | 1.750   |
torch.uint8
+--------------------------------+------------+------------+------------+---------+


Quantizing layer 14/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 26676.564  | -          | -          | 4.567   |
torch.uint8
| self_attn.k_proj-4              | 7712.265   | -          | -          | 4.445   |
torch.uint8
| self_attn.v_proj-4              | 6045.484   | -          | -          | 4.397   |
torch.uint8
| self_attn.o_proj-4              | 733.107    | -          | -          | 4.562   |
torch.uint8
| block_sparse_moe.gate-4         | 39.701     | -          | -          | 3.791   |
torch.uint8
| block_sparse_moe.experts.0.w1-3 | 39977.570  | -          | -          | 4.295   |
torch.int32
| block_sparse_moe.experts.0.w2-3 | 2211.959   | -          | -          | 13.836  |
torch.int32
| block_sparse_moe.experts.0.w3-3 | 42142.938  | -          | -          | 4.299   |
torch.int32
| block_sparse_moe.experts.1.w1-3 | 46357.969  | -          | -          | 4.337   |
torch.int32
| block_sparse_moe.experts.1.w2-3 | 2819.246   | -          | -          | 13.940  |
torch.int32
| block_sparse_moe.experts.1.w3-3 | 48247.648  | -          | -          | 4.293   |
torch.int32
| block_sparse_moe.experts.2.w1-2 | 228922.359 | -          | -          | 3.906   |
torch.uint8
| block_sparse_moe.experts.2.w2-2 | 15489.943  | -          | -          | 12.903  |
torch.uint8
| block_sparse_moe.experts.2.w3-2 | 244423.219 | -          | -          | 3.858   |
torch.uint8
| block_sparse_moe.experts.3.w1-2 | 249148.125 | -          | -          | 3.854   |
torch.uint8
| block_sparse_moe.experts.3.w2-2 | 15992.822  | -          | -          | 12.903  |
torch.uint8
| block_sparse_moe.experts.3.w3-2 | 267712.500 | -          | -          | 3.863   |
torch.uint8
| block_sparse_moe.experts.4.w1-3 | 44130.488  | -          | -          | 4.306   |
torch.int32
| block_sparse_moe.experts.4.w2-3 | 2765.370   | -          | -          | 13.804  |
torch.int32
| block_sparse_moe.experts.4.w3-3 | 49285.859  | -          | -          | 4.273   |
torch.int32
| block_sparse_moe.experts.5.w1-3 | 52093.836  | -          | -          | 4.301   |
torch.int32
| block_sparse_moe.experts.5.w2-3 | 3258.105   | -          | -          | 13.899  |
torch.int32
| block_sparse_moe.experts.5.w3-3 | 55305.445  | -          | -          | 4.317   |
torch.int32
| block_sparse_moe.experts.6.w1-3 | 48711.105  | -          | -          | 4.283   |
torch.int32
| block_sparse_moe.experts.6.w2-3 | 2741.168   | -          | -          | 13.863  |
torch.int32
| block_sparse_moe.experts.6.w3-3 | 50757.430  | -          | -          | 4.313   |
torch.int32
| block_sparse_moe.experts.7.w1-1 | 579174.750 | -          | -          | 1.758   |
torch.uint8
| block_sparse_moe.experts.7.w2-1 | 37828.453  | -          | -          | 6.553   |
torch.uint8
| block_sparse_moe.experts.7.w3-1 | 610397.750 | -          | -          | 1.739   |
torch.uint8
+--------------------------------+------------+------------+------------+---------+


Quantizing layer 15/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 30906.854  | -          | -          | 4.584   |
torch.uint8
| self_attn.k_proj-4              | 7867.921   | -          | -          | 4.465   |
torch.uint8
| self_attn.v_proj-4              | 11130.762  | -          | -          | 4.388   |
torch.uint8
| self_attn.o_proj-4              | 1177.568   | -          | -          | 4.573   |
torch.uint8
| block_sparse_moe.gate-4         | 39.862     | -          | -          | 3.784   |
torch.uint8
| block_sparse_moe.experts.0.w1-2 | 289280.688 | -          | -          | 3.888   |
torch.uint8
| block_sparse_moe.experts.0.w2-2 | 23011.543  | -          | -          | 12.911  |
torch.uint8
| block_sparse_moe.experts.0.w3-2 | 330872.688 | -          | -          | 3.891   |
torch.uint8
| block_sparse_moe.experts.1.w1-3 | 53320.535  | -          | -          | 4.295   |
torch.int32
| block_sparse_moe.experts.1.w2-3 | 3704.202   | -          | -          | 13.834  |
torch.int32
| block_sparse_moe.experts.1.w3-3 | 60574.086  | -          | -          | 4.331   |
torch.int32
| block_sparse_moe.experts.2.w1-1 | 304148.031 | -          | -          | 1.763   |
torch.uint8
| block_sparse_moe.experts.2.w2-1 | 23935.969  | -          | -          | 6.556   |
torch.uint8
| block_sparse_moe.experts.2.w3-1 | 311441.750 | -          | -          | 1.737   |
torch.uint8
| block_sparse_moe.experts.3.w1-3 | 60915.102  | -          | -          | 4.304   |
torch.int32
| block_sparse_moe.experts.3.w2-3 | 4718.008   | -          | -          | 13.868  |
torch.int32
| block_sparse_moe.experts.3.w3-3 | 68362.234  | -          | -          | 4.284   |
torch.int32
| block_sparse_moe.experts.4.w1-2 | 352035.562 | -          | -          | 3.875   |
torch.uint8
| block_sparse_moe.experts.4.w2-2 | 26912.828  | -          | -          | 12.894  |
torch.uint8
| block_sparse_moe.experts.4.w3-2 | 381293.438 | -          | -          | 3.860   |
torch.uint8
| block_sparse_moe.experts.5.w1-3 | 54513.238  | -          | -          | 4.331   |
torch.int32
| block_sparse_moe.experts.5.w2-3 | 3886.673   | -          | -          | 13.803  |
torch.int32
| block_sparse_moe.experts.5.w3-3 | 62262.344  | -          | -          | 4.327   |
torch.int32
| block_sparse_moe.experts.6.w1-3 | 54752.070  | -          | -          | 4.328   |
torch.int32
| block_sparse_moe.experts.6.w2-3 | 3583.171   | -          | -          | 13.792  |
torch.int32
| block_sparse_moe.experts.6.w3-3 | 58290.164  | -          | -          | 4.333   |
torch.int32
| block_sparse_moe.experts.7.w1-3 | 44002.812  | -          | -          | 4.348   |
torch.int32
| block_sparse_moe.experts.7.w2-3 | 3153.766   | -          | -          | 13.928  |
torch.int32
| block_sparse_moe.experts.7.w3-3 | 49383.078  | -          | -          | 4.357   |
torch.int32
+--------------------------------+------------+------------+------------+---------+


Quantizing layer 16/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 40560.016  | -          | -          | 4.633   |
torch.uint8
| self_attn.k_proj-4              | 9600.318   | -          | -          | 4.426   |
torch.uint8
| self_attn.v_proj-4              | 13733.529  | -          | -          | 4.370   |
torch.uint8
| self_attn.o_proj-4              | 1468.128   | -          | -          | 4.545   |
torch.uint8
| block_sparse_moe.gate-4         | 55.309     | -          | -          | 3.775   |
torch.uint8
| block_sparse_moe.experts.0.w1-2 | 357240.750 | -          | -          | 3.876   |
torch.uint8
| block_sparse_moe.experts.0.w2-2 | 30726.992  | -          | -          | 12.917  |
torch.uint8
| block_sparse_moe.experts.0.w3-2 | 398764.188 | -          | -          | 3.866   |
torch.uint8
| block_sparse_moe.experts.1.w1-3 | 72693.930  | -          | -          | 4.312   |
torch.int32
| block_sparse_moe.experts.1.w2-3 | 5504.657   | -          | -          | 13.936  |
torch.int32
| block_sparse_moe.experts.1.w3-3 | 77714.148  | -          | -          | 4.274   |
torch.int32
| block_sparse_moe.experts.2.w1-3 | 70239.836  | -          | -          | 4.293   |
torch.int32
| block_sparse_moe.experts.2.w2-3 | 5784.292   | -          | -          | 13.914  |
torch.int32
| block_sparse_moe.experts.2.w3-3 | 76842.242  | -          | -          | 4.289   |
torch.int32
| block_sparse_moe.experts.3.w1-1 | 346046.438 | -          | -          | 1.749   |
torch.uint8
| block_sparse_moe.experts.3.w2-1 | 29506.850  | -          | -          | 6.622   |
torch.uint8
| block_sparse_moe.experts.3.w3-1 | 350195.219 | -          | -          | 1.768   |
torch.uint8
| block_sparse_moe.experts.4.w1-2 | 384963.219 | -          | -          | 3.871   |
torch.uint8
| block_sparse_moe.experts.4.w2-2 | 29718.916  | -          | -          | 12.932  |
torch.uint8
| block_sparse_moe.experts.4.w3-2 | 407374.875 | -          | -          | 3.852   |
torch.uint8
| block_sparse_moe.experts.5.w1-3 | 58270.543  | -          | -          | 4.334   |
torch.int32
| block_sparse_moe.experts.5.w2-3 | 4175.444   | -          | -          | 13.973  |
torch.int32
| block_sparse_moe.experts.5.w3-3 | 62283.484  | -          | -          | 4.328   |
torch.int32
| block_sparse_moe.experts.6.w1-3 | 58883.305  | -          | -          | 4.323   |
torch.int32
| block_sparse_moe.experts.6.w2-3 | 4970.229   | -          | -          | 13.877  |
torch.int32
| block_sparse_moe.experts.6.w3-3 | 68269.688  | -          | -          | 4.297   |
torch.int32
| block_sparse_moe.experts.7.w1-3 | 68651.859  | -          | -          | 4.337   |
torch.int32
| block_sparse_moe.experts.7.w2-3 | 6172.857   | -          | -          | 13.796  |
torch.int32
| block_sparse_moe.experts.7.w3-3 | 75610.266  | -          | -          | 4.360   |
torch.int32
+--------------------------------+------------+------------+------------+---------+


Quantizing layer 17/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 36313.656  | -          | -          | 4.539   |
torch.uint8
| self_attn.k_proj-4              | 8946.488   | -          | -          | 4.418   |
torch.uint8
| self_attn.v_proj-4              | 12520.730  | -          | -          | 4.444   |
torch.uint8
| self_attn.o_proj-4              | 1683.997   | -          | -          | 4.617   |
torch.uint8
| block_sparse_moe.gate-4         | 54.642     | -          | -          | 3.825   |
torch.uint8
| block_sparse_moe.experts.0.w1-3 | 74614.773  | -          | -          | 4.332   |
torch.int32
| block_sparse_moe.experts.0.w2-3 | 7494.246   | -          | -          | 13.878  |
torch.int32
| block_sparse_moe.experts.0.w3-3 | 79421.938  | -          | -          | 4.347   |
torch.int32
| block_sparse_moe.experts.1.w1-3 | 72474      | -          | -          | 4.302   |
torch.int32
| block_sparse_moe.experts.1.w2-3 | 7106.643   | -          | -          | 13.660  |
torch.int32
| block_sparse_moe.experts.1.w3-3 | 78896.516  | -          | -          | 4.265   |
torch.int32
| block_sparse_moe.experts.2.w1-3 | 72025.906  | -          | -          | 4.307   |
torch.int32
| block_sparse_moe.experts.2.w2-3 | 6392.281   | -          | -          | 13.603  |
torch.int32
| block_sparse_moe.experts.2.w3-3 | 76919.664  | -          | -          | 4.236   |
torch.int32
| block_sparse_moe.experts.3.w1-3 | 77499.508  | -          | -          | 4.190   |
torch.int32
| block_sparse_moe.experts.3.w2-3 | 7643.409   | -          | -          | 13.540  |
torch.int32
| block_sparse_moe.experts.3.w3-3 | 83175.766  | -          | -          | 4.176   |
torch.int32
| block_sparse_moe.experts.4.w1-2 | 436733.625 | -          | -          | 3.799   |
torch.uint8
| block_sparse_moe.experts.4.w2-2 | 40624.445  | -          | -          | 12.603  |
torch.uint8
| block_sparse_moe.experts.4.w3-2 | 480559.688 | -          | -          | 3.730   |
torch.uint8
| block_sparse_moe.experts.5.w1-2 | 377824.688 | -          | -          | 3.805   |
torch.uint8
| block_sparse_moe.experts.5.w2-2 | 41436.676  | -          | -          | 12.693  |
torch.uint8
| block_sparse_moe.experts.5.w3-2 | 432725.219 | -          | -          | 3.760   |
torch.uint8
| block_sparse_moe.experts.6.w1-3 | 84980.891  | -          | -          | 4.237   |
torch.int32
| block_sparse_moe.experts.6.w2-3 | 7983.734   | -          | -          | 13.560  |
torch.int32
| block_sparse_moe.experts.6.w3-3 | 90849.914  | -          | -          | 4.236   |
torch.int32
|                                 | 8          |            |            |         |
torch.uint8
| block_sparse_moe.experts.7.w2-1 | 83436.656  | -          | -          | 6.418   |
torch.uint8
| block_sparse_moe.experts.7.w3-1 | 992882     | -          | -          | 1.701   |
torch.uint8
+--------------------------------+------------+------------+------------+---------+


Quantizing layer 18/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 34989.711  | -          | -          | 4.565   |
torch.uint8
| self_attn.k_proj-4              | 7787.419   | -          | -          | 4.433   |
torch.uint8
| self_attn.v_proj-4              | 13208.189  | -          | -          | 4.401   |
torch.uint8
| self_attn.o_proj-4              | 1893.611   | -          | -          | 4.541   |
torch.uint8
| block_sparse_moe.gate-4         | 55.503     | -          | -          | 3.782   |
torch.uint8
| block_sparse_moe.experts.0.w1-3 | 92864.930  | -          | -          | 4.346   |
torch.int32
| block_sparse_moe.experts.0.w2-3 | 13726.009  | -          | -          | 13.842  |
torch.int32
| block_sparse_moe.experts.0.w3-3 | 103444.758 | -          | -          | 4.366   |
torch.int32
| block_sparse_moe.experts.1.w1-3 | 104699.578 | -          | -          | 4.310   |
torch.int32
| block_sparse_moe.experts.1.w2-3 | 13416.418  | -          | -          | 13.866  |
torch.int32
| block_sparse_moe.experts.1.w3-3 | 118928.828 | -          | -          | 4.330   |
torch.int32
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.2.w2-1 | 133188.312 | -          | -          | 6.552   |
torch.uint8
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.3.w1-3 | 73624.891  | -          | -          | 4.282   |
torch.int32
| block_sparse_moe.experts.3.w2-3 | 9424.705   | -          | -          | 13.869  |
torch.int32
| block_sparse_moe.experts.3.w3-3 | 83019.539  | -          | -          | 4.324   |
torch.int32
| block_sparse_moe.experts.4.w1-3 | 80470.594  | -          | -          | 4.313   |
torch.int32
| block_sparse_moe.experts.4.w2-3 | 9731.830   | -          | -          | 13.817  |
torch.int32
| block_sparse_moe.experts.4.w3-3 | 93788.328  | -          | -          | 4.276   |
torch.int32
| block_sparse_moe.experts.5.w1-3 | 100479.188 | -          | -          | 4.288   |
torch.int32
| block_sparse_moe.experts.5.w2-3 | 11707.684  | -          | -          | 13.968  |
torch.int32
| block_sparse_moe.experts.5.w3-3 | 112042.109 | -          | -          | 4.318   |
torch.int32
| block_sparse_moe.experts.6.w1-2 | 416038.562 | -          | -          | 3.873   |
torch.uint8
| block_sparse_moe.experts.6.w2-2 | 51577.566  | -          | -          | 12.862  |
torch.uint8
| block_sparse_moe.experts.6.w3-2 | 457767.562 | -          | -          | 3.853   |
torch.uint8
| block_sparse_moe.experts.7.w1-2 | 512228.688 | -          | -          | 3.862   |
torch.uint8
| block_sparse_moe.experts.7.w2-2 | 59021.125  | -          | -          | 12.874  |
torch.uint8
| block_sparse_moe.experts.7.w3-2 | 567620.250 | -          | -          | 3.817   |
torch.uint8
+--------------------------------+------------+------------+------------+---------+


Quantizing layer 19/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 43130.684  | -          | -          | 4.599   |
torch.uint8
| self_attn.k_proj-4              | 9396.928   | -          | -          | 4.406   |
torch.uint8
| self_attn.v_proj-4              | 16885.559  | -          | -          | 4.387   |
torch.uint8
| self_attn.o_proj-4              | 2623.651   | -          | -          | 4.573   |
torch.uint8
| block_sparse_moe.gate-4         | 63.206     | -          | -          | 3.768   |
torch.uint8
| block_sparse_moe.experts.0.w1-2 | 545798.625 | -          | -          | 3.874   |
torch.uint8
| block_sparse_moe.experts.0.w2-2 | 89600.344  | -          | -          | 12.949  |
torch.uint8
| block_sparse_moe.experts.0.w3-2 | 659251.625 | -          | -          | 3.843   |
torch.uint8
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.1.w2-1 | 179237.094 | -          | -          | 6.520   |
torch.uint8
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.2.w1-3 | 117530.328 | -          | -          | 4.301   |
torch.int32
| block_sparse_moe.experts.2.w2-3 | 19922.068  | -          | -          | 13.889  |
torch.int32
| block_sparse_moe.experts.2.w3-3 | 132776.906 | -          | -          | 4.300   |
torch.int32
| block_sparse_moe.experts.3.w1-3 | 105603.922 | -          | -          | 4.343   |
torch.int32
| block_sparse_moe.experts.3.w2-3 | 16783.615  | -          | -          | 13.869  |
torch.int32
| block_sparse_moe.experts.3.w3-3 | 122876.898 | -          | -          | 4.391   |
torch.int32
| block_sparse_moe.experts.4.w1-3 | 116548.711 | -          | -          | 4.328   |
torch.int32
| block_sparse_moe.experts.4.w2-3 | 17623.973  | -          | -          | 13.935  |
torch.int32
| block_sparse_moe.experts.4.w3-3 | 130746.672 | -          | -          | 4.338   |
torch.int32
| block_sparse_moe.experts.5.w1-3 | 92676.078  | -          | -          | 4.324   |
torch.int32
| block_sparse_moe.experts.5.w2-3 | 17878.785  | -          | -          | 13.905  |
torch.int32
| block_sparse_moe.experts.5.w3-3 | 108677.727 | -          | -          | 4.350   |
torch.int32
| block_sparse_moe.experts.6.w1-3 | 100544.773 | -          | -          | 4.343   |
torch.int32
| block_sparse_moe.experts.6.w2-3 | 15332.502  | -          | -          | 13.876  |
torch.int32
| block_sparse_moe.experts.6.w3-3 | 116815.703 | -          | -          | 4.332   |
torch.int32
| block_sparse_moe.experts.7.w1-2 | 600815.875 | -          | -          | 3.907   |
torch.uint8
| block_sparse_moe.experts.7.w2-2 | 101868.047 | -          | -          | 12.936  |
torch.uint8
| block_sparse_moe.experts.7.w3-2 | 674083.625 | -          | -          | 3.871   |
torch.uint8
+--------------------------------+------------+------------+------------+---------+


Quantizing layer 20/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 40676.508  | -          | -          | 4.589   |
torch.uint8
| self_attn.k_proj-4              | 8602.583   | -          | -          | 4.383   |
torch.uint8
| self_attn.v_proj-4              | 21021.957  | -          | -          | 4.409   |
torch.uint8
| self_attn.o_proj-4              | 2774.288   | -          | -          | 4.541   |
torch.uint8
| block_sparse_moe.gate-4         | 82.617     | -          | -          | 3.769   |
torch.uint8
| block_sparse_moe.experts.0.w1-3 | 140076.750 | -          | -          | 4.354   |
torch.int32
| block_sparse_moe.experts.0.w2-3 | 24065.256  | -          | -          | 13.904  |
torch.int32
| block_sparse_moe.experts.0.w3-3 | 158468.094 | -          | -          | 4.328   |
torch.int32
| block_sparse_moe.experts.1.w1-2 | 598912.750 | -          | -          | 3.846   |
torch.uint8
| block_sparse_moe.experts.1.w2-2 | 129411.250 | -          | -          | 12.906  |
torch.uint8
| block_sparse_moe.experts.1.w3-2 | 668134.562 | -          | -          | 3.876   |
torch.uint8
| block_sparse_moe.experts.2.w1-3 | 123734.641 | -          | -          | 4.314   |
torch.int32
| block_sparse_moe.experts.2.w2-3 | 21718.637  | -          | -          | 13.931  |
torch.int32
| block_sparse_moe.experts.2.w3-3 | 137200.781 | -          | -          | 4.325   |
torch.int32
| block_sparse_moe.experts.3.w1-2 | 706085.562 | -          | -          | 3.842   |
torch.uint8
| block_sparse_moe.experts.3.w2-2 | 136041.906 | -          | -          | 12.934  |
torch.uint8
| block_sparse_moe.experts.3.w3-2 | 789203.250 | -          | -          | 3.856   |
torch.uint8
| block_sparse_moe.experts.4.w1-3 | 114097.172 | -          | -          | 4.328   |
torch.int32
| block_sparse_moe.experts.4.w2-3 | 21578.016  | -          | -          | 13.879  |
torch.int32
| block_sparse_moe.experts.4.w3-3 | 138667.969 | -          | -          | 4.290   |
torch.int32
| block_sparse_moe.experts.5.w1-3 | 135108.844 | -          | -          | 4.303   |
torch.int32
| block_sparse_moe.experts.5.w2-3 | 22769.545  | -          | -          | 13.881  |
torch.int32
| block_sparse_moe.experts.5.w3-3 | 149157.359 | -          | -          | 4.329   |
torch.int32
| block_sparse_moe.experts.6.w1-3 | 123870.258 | -          | -          | 4.322   |
torch.int32
| block_sparse_moe.experts.6.w2-3 | 23657.865  | -          | -          | 13.845  |
torch.int32
| block_sparse_moe.experts.6.w3-3 | 141772.250 | -          | -          | 4.318   |
torch.int32
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.7.w2-1 | 231114.781 | -          | -          | 6.596   |
torch.uint8
|                                 | 0          |            |            |         |
torch.uint8
+--------------------------------+------------+------------+------------+---------+


Quantizing layer 21/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 42125.277  | -          | -          | 4.576   |
torch.uint8
| self_attn.k_proj-4              | 8508.977   | -          | -          | 4.416   |
torch.uint8
| self_attn.v_proj-4              | 23285.551  | -          | -          | 4.440   |
torch.uint8
| self_attn.o_proj-4              | 3042.813   | -          | -          | 4.586   |
torch.uint8
| block_sparse_moe.gate-4         | 105.632    | -          | -          | 3.861   |
torch.uint8
| block_sparse_moe.experts.0.w1-2 | 705360     | -          | -          | 3.897   |
torch.uint8
| block_sparse_moe.experts.0.w2-2 | 113462.148 | -          | -          | 12.932  |
torch.uint8
| block_sparse_moe.experts.0.w3-2 | 785226.500 | -          | -          | 3.868   |
torch.uint8
| block_sparse_moe.experts.1.w1-3 | 150168.859 | -          | -          | 4.281   |
torch.int32
| block_sparse_moe.experts.1.w2-3 | 25350.523  | -          | -          | 13.906  |
torch.int32
| block_sparse_moe.experts.1.w3-3 | 166366.766 | -          | -          | 4.369   |
torch.int32
| block_sparse_moe.experts.2.w1-3 | 140465.625 | -          | -          | 4.310   |
torch.int32
| block_sparse_moe.experts.2.w2-3 | 22253.242  | -          | -          | 13.878  |
torch.int32
| block_sparse_moe.experts.2.w3-3 | 153461.750 | -          | -          | 4.339   |
torch.int32
|                                 | 5          |            |            |         |
torch.uint8
| block_sparse_moe.experts.3.w2-1 | 230395.656 | -          | -          | 6.501   |
torch.uint8
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.4.w1-3 | 154603.797 | -          | -          | 4.334   |
torch.int32
| block_sparse_moe.experts.4.w2-3 | 23181.424  | -          | -          | 13.883  |
torch.int32
| block_sparse_moe.experts.4.w3-3 | 170611.047 | -          | -          | 4.310   |
torch.int32
| block_sparse_moe.experts.5.w1-3 | 138696.938 | -          | -          | 4.321   |
torch.int32
| block_sparse_moe.experts.5.w2-3 | 24361.910  | -          | -          | 13.840  |
torch.int32
| block_sparse_moe.experts.5.w3-3 | 159259.625 | -          | -          | 4.336   |
torch.int32
| block_sparse_moe.experts.6.w1-2 | 919874.688 | -          | -          | 3.831   |
torch.uint8
| block_sparse_moe.experts.6.w2-2 | 148682.859 | -          | -          | 12.874  |
torch.uint8
|                                 | 8          |            |            |         |
torch.uint8
| block_sparse_moe.experts.7.w1-3 | 162946.859 | -          | -          | 4.312   |
torch.int32
| block_sparse_moe.experts.7.w2-3 | 25685.104  | -          | -          | 14.031  |
torch.int32
| block_sparse_moe.experts.7.w3-3 | 176940.703 | -          | -          | 4.358   |
torch.int32
+--------------------------------+------------+------------+------------+---------+


Quantizing layer 22/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 41789.906  | -          | -          | 4.532   |
torch.uint8
| self_attn.k_proj-4              | 8448.152   | -          | -          | 4.404   |
torch.uint8
| self_attn.v_proj-4              | 23433.961  | -          | -          | 4.419   |
torch.uint8
| self_attn.o_proj-4              | 2920.838   | -          | -          | 4.524   |
torch.uint8
| block_sparse_moe.gate-4         | 132.826    | -          | -          | 3.804   |
torch.uint8
| block_sparse_moe.experts.0.w1-2 | 944683.875 | -          | -          | 3.887   |
torch.uint8
| block_sparse_moe.experts.0.w2-2 | 148396.516 | -          | -          | 12.950  |
torch.uint8
|                                 | 5          |            |            |         |
torch.uint8
| block_sparse_moe.experts.1.w1-3 | 149931.359 | -          | -          | 4.339   |
torch.int32
| block_sparse_moe.experts.1.w2-3 | 21788.895  | -          | -          | 13.858  |
torch.int32
| block_sparse_moe.experts.1.w3-3 | 162644.172 | -          | -          | 4.365   |
torch.int32
|                                 | 5          |            |            |         |
torch.uint8
| block_sparse_moe.experts.2.w2-1 | 249484.156 | -          | -          | 6.463   |
torch.uint8
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.3.w1-3 | 193967.516 | -          | -          | 4.282   |
torch.int32
| block_sparse_moe.experts.3.w2-3 | 25416.516  | -          | -          | 13.903  |
torch.int32
| block_sparse_moe.experts.3.w3-3 | 204732.266 | -          | -          | 4.282   |
torch.int32
| block_sparse_moe.experts.4.w1-3 | 177531.312 | -          | -          | 4.348   |
torch.int32
| block_sparse_moe.experts.4.w2-3 | 26138.391  | -          | -          | 13.988  |
torch.int32
| block_sparse_moe.experts.4.w3-3 | 189225.156 | -          | -          | 4.320   |
torch.int32
| block_sparse_moe.experts.5.w1-2 | 830069.625 | -          | -          | 3.852   |
torch.uint8
| block_sparse_moe.experts.5.w2-2 | 114528.594 | -          | -          | 12.815  |
torch.uint8
| block_sparse_moe.experts.5.w3-2 | 888194.375 | -          | -          | 3.845   |
torch.uint8
| block_sparse_moe.experts.6.w1-3 | 198562.953 | -          | -          | 4.307   |
torch.int32
| block_sparse_moe.experts.6.w2-3 | 27609.859  | -          | -          | 13.861  |
torch.int32
| block_sparse_moe.experts.6.w3-3 | 210447.922 | -          | -          | 4.321   |
torch.int32
| block_sparse_moe.experts.7.w1-3 | 182377.828 | -          | -          | 4.315   |
torch.int32
| block_sparse_moe.experts.7.w2-3 | 26985.730  | -          | -          | 14.017  |
torch.int32
| block_sparse_moe.experts.7.w3-3 | 193772.344 | -          | -          | 4.326   |
torch.int32
+--------------------------------+------------+------------+------------+---------+


Quantizing layer 23/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 40235.996  | -          | -          | 4.539   |
torch.uint8
| self_attn.k_proj-4              | 7843.354   | -          | -          | 4.420   |
torch.uint8
| self_attn.v_proj-4              | 23305.826  | -          | -          | 4.420   |
torch.uint8
| self_attn.o_proj-4              | 1867.072   | -          | -          | 4.490   |
torch.uint8
| block_sparse_moe.gate-4         | 137.871    | -          | -          | 3.707   |
torch.uint8
| block_sparse_moe.experts.0.w1-2 | 1137102    | -          | -          | 3.768   |
torch.uint8
| block_sparse_moe.experts.0.w2-2 | 150933.422 | -          | -          | 12.652  |
torch.uint8
| block_sparse_moe.experts.0.w3-2 | 1193738    | -          | -          | 3.757   |
torch.uint8
| block_sparse_moe.experts.1.w1-3 | 188155.531 | -          | -          | 4.202   |
torch.int32
| block_sparse_moe.experts.1.w2-3 | 28209.312  | -          | -          | 13.636  |
torch.int32
| block_sparse_moe.experts.1.w3-3 | 200585.062 | -          | -          | 4.223   |
torch.int32
| block_sparse_moe.experts.2.w1-3 | 164476.812 | -          | -          | 4.268   |
torch.int32
| block_sparse_moe.experts.2.w2-3 | 22803.682  | -          | -          | 13.681  |
torch.int32
| block_sparse_moe.experts.2.w3-3 | 174074.203 | -          | -          | 4.215   |
torch.int32
| block_sparse_moe.experts.3.w1-3 | 214114.531 | -          | -          | 4.176   |
torch.int32
| block_sparse_moe.experts.3.w2-3 | 32369.395  | -          | -          | 13.542  |
torch.int32
| block_sparse_moe.experts.3.w3-3 | 226713.031 | -          | -          | 4.214   |
torch.int32
| block_sparse_moe.experts.4.w1-3 | 188233.188 | -          | -          | 4.222   |
torch.int32
| block_sparse_moe.experts.4.w2-3 | 26814.160  | -          | -          | 13.701  |
torch.int32
| block_sparse_moe.experts.4.w3-3 | 198723.984 | -          | -          | 4.290   |
torch.int32
|                                 | 5          |            |            |         |
torch.uint8
| block_sparse_moe.experts.5.w2-2 | 148410.984 | -          | -          | 12.925  |
torch.uint8
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.6.w1-1 | 2170776    | -          | -          | 1.761   |
torch.uint8
| block_sparse_moe.experts.6.w2-1 | 281154.938 | -          | -          | 6.580   |
torch.uint8
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.7.w1-3 | 184614.844 | -          | -          | 4.318   |
torch.int32
| block_sparse_moe.experts.7.w2-3 | 27429.414  | -          | -          | 13.973  |
torch.int32
| block_sparse_moe.experts.7.w3-3 | 197929.625 | -          | -          | 4.260   |
torch.int32
+--------------------------------+------------+------------+------------+---------+


Quantizing layer 24/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 40186.953  | -          | -          | 4.532   |
torch.uint8
| self_attn.k_proj-4              | 7774.705   | -          | -          | 4.416   |
torch.uint8
| self_attn.v_proj-4              | 23185.855  | -          | -          | 4.453   |
torch.uint8
| self_attn.o_proj-4              | 2848.728   | -          | -          | 4.561   |
torch.uint8
| block_sparse_moe.gate-4         | 182.709    | -          | -          | 3.854   |
torch.uint8
| block_sparse_moe.experts.0.w1-3 | 240448.594 | -          | -          | 4.311   |
torch.int32
| block_sparse_moe.experts.0.w2-3 | 37867.367  | -          | -          | 13.882  |
torch.int32
| block_sparse_moe.experts.0.w3-3 | 258932.250 | -          | -          | 4.312   |
torch.int32
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.1.w2-2 | 211178.219 | -          | -          | 12.849  |
torch.uint8
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.2.w1-3 | 200498.031 | -          | -          | 4.310   |
torch.int32
| block_sparse_moe.experts.2.w2-3 | 29364.145  | -          | -          | 13.932  |
torch.int32
| block_sparse_moe.experts.2.w3-3 | 215188.766 | -          | -          | 4.317   |
torch.int32
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.3.w2-1 | 292258.875 | -          | -          | 6.541   |
torch.uint8
|                                 | 0          |            |            |         |
torch.uint8
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.4.w2-2 | 153961.609 | -          | -          | 12.793  |
torch.uint8
|                                 | 5          |            |            |         |
torch.uint8
| block_sparse_moe.experts.5.w1-3 | 197283.844 | -          | -          | 4.295   |
torch.int32
| block_sparse_moe.experts.5.w2-3 | 29119.434  | -          | -          | 13.876  |
torch.int32
| block_sparse_moe.experts.5.w3-3 | 210725.828 | -          | -          | 4.294   |
torch.int32
| block_sparse_moe.experts.6.w1-3 | 223025.719 | -          | -          | 4.300   |
torch.int32
| block_sparse_moe.experts.6.w2-3 | 33863.266  | -          | -          | 13.879  |
torch.int32
| block_sparse_moe.experts.6.w3-3 | 239289.594 | -          | -          | 4.308   |
torch.int32
| block_sparse_moe.experts.7.w1-3 | 193158.328 | -          | -          | 4.364   |
torch.int32
| block_sparse_moe.experts.7.w2-3 | 28426.133  | -          | -          | 13.852  |
torch.int32
| block_sparse_moe.experts.7.w3-3 | 209384.812 | -          | -          | 4.350   |
torch.int32
+--------------------------------+------------+------------+------------+---------+


Quantizing layer 25/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 45157.484  | -          | -          | 4.541   |
torch.uint8
| self_attn.k_proj-4              | 8833.665   | -          | -          | 4.438   |
torch.uint8
| self_attn.v_proj-4              | 28089.281  | -          | -          | 4.401   |
torch.uint8
| self_attn.o_proj-4              | 2720.084   | -          | -          | 4.563   |
torch.uint8
| block_sparse_moe.gate-4         | 192.952    | -          | -          | 3.827   |
torch.uint8
| block_sparse_moe.experts.0.w1-3 | 238573.422 | -          | -          | 4.356   |
torch.int32
| block_sparse_moe.experts.0.w2-3 | 37410.781  | -          | -          | 13.843  |
torch.int32
| block_sparse_moe.experts.0.w3-3 | 256041.266 | -          | -          | 4.349   |
torch.int32
| block_sparse_moe.experts.1.w1-3 | 242803.625 | -          | -          | 4.365   |
torch.int32
| block_sparse_moe.experts.1.w2-3 | 34940.680  | -          | -          | 13.946  |
torch.int32
| block_sparse_moe.experts.1.w3-3 | 259671.219 | -          | -          | 4.335   |
torch.int32
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.2.w2-2 | 207387.766 | -          | -          | 12.875  |
torch.uint8
|                                 | 5          |            |            |         |
torch.uint8
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.3.w2-2 | 184494.906 | -          | -          | 12.906  |
torch.uint8
| block_sparse_moe.experts.3.w3-2 | 1311158    | -          | -          | 3.849   |
torch.uint8
| block_sparse_moe.experts.4.w1-3 | 227639.328 | -          | -          | 4.299   |
torch.int32
| block_sparse_moe.experts.4.w2-3 | 33840.020  | -          | -          | 13.983  |
torch.int32
| block_sparse_moe.experts.4.w3-3 | 241739.375 | -          | -          | 4.338   |
torch.int32
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.5.w2-1 | 376389.938 | -          | -          | 6.536   |
torch.uint8
| block_sparse_moe.experts.5.w3-1 | 2774964    | -          | -          | 1.779   |
torch.uint8
| block_sparse_moe.experts.6.w1-3 | 246017.562 | -          | -          | 4.307   |
torch.int32
| block_sparse_moe.experts.6.w2-3 | 35550.094  | -          | -          | 13.940  |
torch.int32
| block_sparse_moe.experts.6.w3-3 | 264016     | -          | -          | 4.371   |
torch.int32
| block_sparse_moe.experts.7.w1-3 | 229955.766 | -          | -          | 4.296   |
torch.int32
| block_sparse_moe.experts.7.w2-3 | 31479.852  | -          | -          | 14.071  |
torch.int32
| block_sparse_moe.experts.7.w3-3 | 246474.781 | -          | -          | 4.357   |
torch.int32
+--------------------------------+------------+------------+------------+---------+


Quantizing layer 26/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 47365.320  | -          | -          | 4.546   |
torch.uint8
| self_attn.k_proj-4              | 8724.750   | -          | -          | 4.375   |
torch.uint8
| self_attn.v_proj-4              | 32113.898  | -          | -          | 4.393   |
torch.uint8
| self_attn.o_proj-4              | 3187.853   | -          | -          | 4.578   |
torch.uint8
| block_sparse_moe.gate-4         | 195.261    | -          | -          | 3.819   |
torch.uint8
| block_sparse_moe.experts.0.w1-3 | 250455.750 | -          | -          | 4.298   |
torch.int32
| block_sparse_moe.experts.0.w2-3 | 41892.355  | -          | -          | 13.891  |
torch.int32
| block_sparse_moe.experts.0.w3-3 | 273408.500 | -          | -          | 4.312   |
torch.int32
| block_sparse_moe.experts.1.w1-3 | 237111.500 | -          | -          | 4.301   |
torch.int32
| block_sparse_moe.experts.1.w2-3 | 37012.086  | -          | -          | 13.979  |
torch.int32
| block_sparse_moe.experts.1.w3-3 | 254148.688 | -          | -          | 4.344   |
torch.int32
| block_sparse_moe.experts.2.w1-3 | 251342.969 | -          | -          | 4.334   |
torch.int32
| block_sparse_moe.experts.2.w2-3 | 39113.555  | -          | -          | 13.874  |
torch.int32
| block_sparse_moe.experts.2.w3-3 | 270445.938 | -          | -          | 4.361   |
torch.int32
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.3.w2-2 | 279444.125 | -          | -          | 12.981  |
torch.uint8
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.4.w1-3 | 276431.875 | -          | -          | 4.317   |
torch.int32
| block_sparse_moe.experts.4.w2-3 | 46872.320  | -          | -          | 13.931  |
torch.int32
| block_sparse_moe.experts.4.w3-3 | 300946.438 | -          | -          | 4.323   |
torch.int32
| block_sparse_moe.experts.5.w1-3 | 274173.438 | -          | -          | 4.341   |
torch.int32
| block_sparse_moe.experts.5.w2-3 | 46329.613  | -          | -          | 13.899  |
torch.int32
| block_sparse_moe.experts.5.w3-3 | 299293.375 | -          | -          | 4.306   |
torch.int32
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.6.w2-1 | 427633.531 | -          | -          | 6.544   |
torch.uint8
|                                 | 0          |            |            |         |
torch.uint8
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.7.w2-2 | 224213.703 | -          | -          | 12.929  |
torch.uint8
|                                 | 0          |            |            |         |
torch.uint8
+--------------------------------+------------+------------+------------+---------+


Quantizing layer 27/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 47079.820  | -          | -          | 4.557   |
torch.uint8
| self_attn.k_proj-4              | 8479.251   | -          | -          | 4.379   |
torch.uint8
| self_attn.v_proj-4              | 36283.320  | -          | -          | 4.478   |
torch.uint8
| self_attn.o_proj-4              | 4220.816   | -          | -          | 4.545   |
torch.uint8
| block_sparse_moe.gate-4         | 193.404    | -          | -          | 3.779   |
torch.uint8
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.0.w2-2 | 324630.938 | -          | -          | 12.984  |
torch.uint8
|                                 | 5          |            |            |         |
torch.uint8
| block_sparse_moe.experts.1.w1-3 | 271099.281 | -          | -          | 4.352   |
torch.int32
| block_sparse_moe.experts.1.w2-3 | 49932.531  | -          | -          | 13.884  |
torch.int32
| block_sparse_moe.experts.1.w3-3 | 299051.562 | -          | -          | 4.297   |
torch.int32
| block_sparse_moe.experts.2.w1-3 | 311529.375 | -          | -          | 4.299   |
torch.int32
| block_sparse_moe.experts.2.w2-3 | 59397.859  | -          | -          | 14.020  |
torch.int32
| block_sparse_moe.experts.2.w3-3 | 343543.500 | -          | -          | 4.325   |
torch.int32
| block_sparse_moe.experts.3.w1-3 | 296875.375 | -          | -          | 4.241   |
torch.int32
| block_sparse_moe.experts.3.w2-3 | 56139.570  | -          | -          | 13.841  |
torch.int32
| block_sparse_moe.experts.3.w3-3 | 329489.875 | -          | -          | 4.325   |
torch.int32
| block_sparse_moe.experts.4.w1-3 | 274792     | -          | -          | 4.299   |
torch.int32
| block_sparse_moe.experts.4.w2-3 | 53616.188  | -          | -          | 13.960  |
torch.int32
| block_sparse_moe.experts.4.w3-3 | 305993.312 | -          | -          | 4.287   |
torch.int32
|                                 | 5          |            |            |         |
torch.uint8
| block_sparse_moe.experts.5.w2-2 | 233602.844 | -          | -          | 12.844  |
torch.uint8
|                                 | 5          |            |            |         |
torch.uint8
| block_sparse_moe.experts.6.w1-1 | 3238380    | -          | -          | 1.730   |
torch.uint8
| block_sparse_moe.experts.6.w2-1 | 664234.375 | -          | -          | 6.520   |
torch.uint8
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.7.w1-3 | 242628.719 | -          | -          | 4.308   |
torch.int32
| block_sparse_moe.experts.7.w2-3 | 42463.855  | -          | -          | 14.035  |
torch.int32
| block_sparse_moe.experts.7.w3-3 | 265783.375 | -          | -          | 4.281   |
torch.int32
+--------------------------------+------------+------------+------------+---------+


Quantizing layer 28/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 47060.570  | -          | -          | 4.542   |
torch.uint8
| self_attn.k_proj-4              | 9008.915   | -          | -          | 4.393   |
torch.uint8
| self_attn.v_proj-4              | 32062.973  | -          | -          | 4.435   |
torch.uint8
| self_attn.o_proj-4              | 4603.173   | -          | -          | 4.564   |
torch.uint8
| block_sparse_moe.gate-4         | 171.890    | -          | -          | 3.854   |
torch.uint8
| block_sparse_moe.experts.0.w1-3 | 312476.406 | -          | -          | 4.359   |
torch.int32
| block_sparse_moe.experts.0.w2-3 | 60578.461  | -          | -          | 14.023  |
torch.int32
| block_sparse_moe.experts.0.w3-3 | 345675.188 | -          | -          | 4.342   |
torch.int32
| block_sparse_moe.experts.1.w1-2 | 1543351    | -          | -          | 3.878   |
torch.uint8
| block_sparse_moe.experts.1.w2-2 | 338466.375 | -          | -          | 12.891  |
torch.uint8
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.2.w1-3 | 310821.656 | -          | -          | 4.338   |
torch.int32
| block_sparse_moe.experts.2.w2-3 | 66893.719  | -          | -          | 13.974  |
torch.int32
| block_sparse_moe.experts.2.w3-3 | 350944.812 | -          | -          | 4.347   |
torch.int32
| block_sparse_moe.experts.3.w1-1 | 3191987    | -          | -          | 1.769   |
torch.uint8
| block_sparse_moe.experts.3.w2-1 | 696701.750 | -          | -          | 6.509   |
torch.uint8
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.4.w1-3 | 309555.062 | -          | -          | 4.354   |
torch.int32
| block_sparse_moe.experts.4.w2-3 | 67847.492  | -          | -          | 13.961  |
torch.int32
| block_sparse_moe.experts.4.w3-3 | 347669.156 | -          | -          | 4.367   |
torch.int32
| block_sparse_moe.experts.5.w1-3 | 325932.875 | -          | -          | 4.340   |
torch.int32
| block_sparse_moe.experts.5.w2-3 | 73793.016  | -          | -          | 14.040  |
torch.int32
| block_sparse_moe.experts.5.w3-3 | 368403.875 | -          | -          | 4.336   |
torch.int32
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.6.w2-2 | 392452.688 | -          | -          | 13.025  |
torch.uint8
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.7.w1-3 | 294180.844 | -          | -          | 4.314   |
torch.int32
| block_sparse_moe.experts.7.w2-3 | 60092.312  | -          | -          | 13.654  |
torch.int32
| block_sparse_moe.experts.7.w3-3 | 325020.500 | -          | -          | 4.290   |
torch.int32
+--------------------------------+------------+------------+------------+---------+


Quantizing layer 29/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 44534.602  | -          | -          | 4.470   |
torch.uint8
| self_attn.k_proj-4              | 7383.710   | -          | -          | 4.306   |
torch.uint8
| self_attn.v_proj-4              | 55398.918  | -          | -          | 4.328   |
torch.uint8
| self_attn.o_proj-4              | 10375.104  | -          | -          | 4.471   |
torch.uint8
| block_sparse_moe.gate-4         | 163.382    | -          | -          | 3.696   |
torch.uint8
| block_sparse_moe.experts.0.w1-1 | 4044277    | -          | -          | 1.763   |
torch.uint8
|                                 | 5          |            |            |         |
torch.uint8
| block_sparse_moe.experts.0.w3-1 | 4469265    | -          | -          | 1.692   |
torch.uint8
| block_sparse_moe.experts.1.w1-3 | 332489.906 | -          | -          | 4.197   |
torch.int32
| block_sparse_moe.experts.1.w2-3 | 76868.203  | -          | -          | 13.622  |
torch.int32
| block_sparse_moe.experts.1.w3-3 | 381523.688 | -          | -          | 4.337   |
torch.int32
|                                 | 5          |            |            |         |
torch.uint8
| block_sparse_moe.experts.2.w2-2 | 480889.375 | -          | -          | 12.897  |
torch.uint8
| block_sparse_moe.experts.2.w3-2 | 2291926    | -          | -          | 3.864   |
torch.uint8
| block_sparse_moe.experts.3.w1-3 | 336127.438 | -          | -          | 4.301   |
torch.int32
| block_sparse_moe.experts.3.w2-3 | 89386.008  | -          | -          | 13.839  |
torch.int32
| block_sparse_moe.experts.3.w3-3 | 384647.906 | -          | -          | 4.269   |
torch.int32
| block_sparse_moe.experts.4.w1-3 | 321762.719 | -          | -          | 4.360   |
torch.int32
| block_sparse_moe.experts.4.w2-3 | 91745.312  | -          | -          | 14.815  |
torch.int32
| block_sparse_moe.experts.4.w3-3 | 372619.594 | -          | -          | 4.485   |
torch.int32
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.5.w2-2 | 588809.750 | -          | -          | 12.882  |
torch.uint8
| block_sparse_moe.experts.5.w3-2 | 2266938    | -          | -          | 3.877   |
torch.uint8
| block_sparse_moe.experts.6.w1-3 | 369849.688 | -          | -          | 4.317   |
torch.int32
| block_sparse_moe.experts.6.w2-3 | 104977.766 | -          | -          | 13.912  |
torch.int32
| block_sparse_moe.experts.6.w3-3 | 423989.812 | -          | -          | 4.308   |
torch.int32
| block_sparse_moe.experts.7.w1-3 | 313855.094 | -          | -          | 4.323   |
torch.int32
| block_sparse_moe.experts.7.w2-3 | 88531.891  | -          | -          | 13.895  |
torch.int32
| block_sparse_moe.experts.7.w3-3 | 365616.250 | -          | -          | 4.290   |
torch.int32
+--------------------------------+------------+------------+------------+---------+


Quantizing layer 30/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 50302.609  | -          | -          | 4.600   |
torch.uint8
| self_attn.k_proj-4              | 7693.634   | -          | -          | 4.433   |
torch.uint8
| self_attn.v_proj-4              | 86218.773  | -          | -          | 4.426   |
torch.uint8
| self_attn.o_proj-4              | 21254.125  | -          | -          | 4.575   |
torch.uint8
| block_sparse_moe.gate-4         | 116.390    | -          | -          | 3.752   |
torch.uint8
|                                 | 0          |            |            |         |
torch.uint8
|                                 | 5          |            |            |         |
torch.uint8
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.1.w1-3 | 387880.469 | -          | -          | 4.275   |
torch.int32
| block_sparse_moe.experts.1.w2-3 | 152146.844 | -          | -          | 13.785  |
torch.int32
| block_sparse_moe.experts.1.w3-3 | 449503.312 | -          | -          | 4.311   |
torch.int32
| block_sparse_moe.experts.2.w1-3 | 394685.344 | -          | -          | 4.310   |
torch.int32
| block_sparse_moe.experts.2.w2-3 | 142565.734 | -          | -          | 13.946  |
torch.int32
| block_sparse_moe.experts.2.w3-3 | 457504.156 | -          | -          | 4.398   |
torch.int32
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.3.w2-2 | 661679.625 | -          | -          | 12.851  |
torch.uint8
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.4.w1-3 | 297015.969 | -          | -          | 4.303   |
torch.int32
| block_sparse_moe.experts.4.w2-3 | 105397.695 | -          | -          | 13.844  |
torch.int32
| block_sparse_moe.experts.4.w3-3 | 348523.188 | -          | -          | 4.307   |
torch.int32
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.5.w2-2 | 441718.094 | -          | -          | 12.938  |
torch.uint8
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.6.w1-3 | 356930.969 | -          | -          | 4.310   |
torch.int32
| block_sparse_moe.experts.6.w2-3 | 123300.859 | -          | -          | 13.940  |
torch.int32
| block_sparse_moe.experts.6.w3-3 | 415304     | -          | -          | 4.353   |
torch.int32
| block_sparse_moe.experts.7.w1-3 | 370815.062 | -          | -          | 4.317   |
torch.int32
| block_sparse_moe.experts.7.w2-3 | 168324.406 | -          | -          | 13.944  |
torch.int32
| block_sparse_moe.experts.7.w3-3 | 431648.250 | -          | -          | 4.280   |
torch.int32
+--------------------------------+------------+------------+------------+---------+


Quantizing layer 31/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 49604.066  | -          | -          | 4.602   |
torch.uint8
| self_attn.k_proj-4              | 7041.741   | -          | -          | 4.368   |
torch.uint8
| self_attn.v_proj-4              | 99618.203  | -          | -          | 4.473   |
torch.uint8
| self_attn.o_proj-4              | 26258.895  | -          | -          | 4.560   |
torch.uint8
| block_sparse_moe.gate-4         | 101.463    | -          | -          | 3.737   |
torch.uint8
|                                 | 0          |            |            |         |
torch.uint8
|                                 | 0          |            |            |         |
torch.uint8
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.1.w1-3 | 264675.219 | -          | -          | 4.257   |
torch.int32
| block_sparse_moe.experts.1.w2-3 | 92670.531  | -          | -          | 13.801  |
torch.int32
| block_sparse_moe.experts.1.w3-3 | 301785.062 | -          | -          | 4.267   |
torch.int32
| block_sparse_moe.experts.2.w1-3 | 304881.125 | -          | -          | 4.275   |
torch.int32
| block_sparse_moe.experts.2.w2-3 | 131033.969 | -          | -          | 13.796  |
torch.int32
| block_sparse_moe.experts.2.w3-3 | 351647.500 | -          | -          | 4.359   |
torch.int32
| block_sparse_moe.experts.3.w1-3 | 431979.125 | -          | -          | 4.270   |
torch.int32
| block_sparse_moe.experts.3.w2-3 | 231462.062 | -          | -          | 13.804  |
torch.int32
| block_sparse_moe.experts.3.w3-3 | 501027.531 | -          | -          | 4.331   |
torch.int32
| block_sparse_moe.experts.4.w1-2 | 1818230    | -          | -          | 3.873   |
torch.uint8
| block_sparse_moe.experts.4.w2-2 | 772162.250 | -          | -          | 12.892  |
torch.uint8
|                                 | 5          |            |            |         |
torch.uint8
| block_sparse_moe.experts.5.w1-2 | 2165499    | -          | -          | 3.834   |
torch.uint8
|                                 | 5          |            |            |         |
torch.uint8
| block_sparse_moe.experts.5.w3-2 | 2555693    | -          | -          | 3.859   |
torch.uint8
| block_sparse_moe.experts.6.w1-3 | 248141.250 | -          | -          | 4.260   |
torch.int32
| block_sparse_moe.experts.6.w2-3 | 96881.523  | -          | -          | 13.882  |
torch.int32
| block_sparse_moe.experts.6.w3-3 | 283767.969 | -          | -          | 4.336   |
torch.int32
| block_sparse_moe.experts.7.w1-3 | 401111.562 | -          | -          | 4.275   |
torch.int32
| block_sparse_moe.experts.7.w2-3 | 207489.203 | -          | -          | 13.970  |
torch.int32
| block_sparse_moe.experts.7.w3-3 | 455750.438 | -          | -          | 4.287   |
torch.int32
+--------------------------------+------------+------------+------------+---------+


Quantizing layer 32/32..
+--------------------------------+------------+------------+------------+---------+
|              name              |weight_error| fp_inp_SNR | q_inp_SNR  |  time   |
+================================+============+============+============+=========+
| self_attn.q_proj-4              | 45315.383  | -          | -          | 4.515   |
torch.uint8
| self_attn.k_proj-4              | 6453.051   | -          | -          | 4.383   |
torch.uint8
| self_attn.v_proj-4              | 90205.047  | -          | -          | 4.479   |
torch.uint8
| self_attn.o_proj-4              | 24930.027  | -          | -          | 4.581   |
torch.uint8
| block_sparse_moe.gate-4         | 87.674     | -          | -          | 3.793   |
torch.uint8
| block_sparse_moe.experts.0.w1-2 | 984868.688 | -          | -          | 3.874   |
torch.uint8
| block_sparse_moe.experts.0.w2-2 | 614793.250 | -          | -          | 12.932  |
torch.uint8
| block_sparse_moe.experts.0.w3-2 | 1111749    | -          | -          | 3.848   |
torch.uint8
| block_sparse_moe.experts.1.w1-3 | 381031.344 | -          | -          | 4.298   |
torch.int32
| block_sparse_moe.experts.1.w2-3 | 527538.938 | -          | -          | 13.942  |
torch.int32
| block_sparse_moe.experts.1.w3-3 | 420500.812 | -          | -          | 4.342   |
torch.int32
| block_sparse_moe.experts.2.w1-3 | 221963     | -          | -          | 4.347   |
torch.int32
| block_sparse_moe.experts.2.w2-3 | 194378.406 | -          | -          | 13.806  |
torch.int32
| block_sparse_moe.experts.2.w3-3 | 248826.344 | -          | -          | 4.325   |
torch.int32
| block_sparse_moe.experts.3.w1-3 | 214569.312 | -          | -          | 4.337   |
torch.int32
| block_sparse_moe.experts.3.w2-3 | 117063.250 | -          | -          | 13.896  |
torch.int32
| block_sparse_moe.experts.3.w3-3 | 227633.938 | -          | -          | 4.315   |
torch.int32
| block_sparse_moe.experts.4.w1-2 | 922907     | -          | -          | 3.870   |
torch.uint8
| block_sparse_moe.experts.4.w2-2 | 504208.938 | -          | -          | 13.052  |
torch.uint8
|                                 | 5          |            |            |         |
torch.uint8
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.5.w2-2 | 2602756    | -          | -          | 13.029  |
torch.uint8
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.6.w1-3 | 174213.406 | -          | -          | 4.275   |
torch.int32
| block_sparse_moe.experts.6.w2-3 | 90448.836  | -          | -          | 13.922  |
torch.int32
| block_sparse_moe.experts.6.w3-3 | 191907.031 | -          | -          | 4.300   |
torch.int32
|                                 | 0          |            |            |         |
torch.uint8
| block_sparse_moe.experts.7.w2-2 | 1125323    | -          | -          | 13.065  |
torch.uint8
|                                 | 0          |            |            |         |
torch.uint8
+--------------------------------+------------+------------+------------+---------+


quantization time: 6833.4650955200195 s
MixtralForCausalLM(
  (model): MixtralModel(
    (embed_tokens): Embedding(32000, 4096)
    (layers): ModuleList(
      (0-31): 32 x MixtralDecoderLayer(
        (self_attn): MixtralAttention(
          (q_proj): QLinear(in_features=4096, out_features=4096, bias=False)
          (k_proj): QLinear(in_features=4096, out_features=1024, bias=False)
          (v_proj): QLinear(in_features=4096, out_features=1024, bias=False)
          (o_proj): QLinear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): MixtralRotaryEmbedding()
        )
        (block_sparse_moe): MixtralSparseMoeBlock(
          (gate): QLinear(in_features=4096, out_features=8, bias=False)
          (experts): ModuleList(
            (0-7): 8 x MixtralBLockSparseTop2MLP(
              (w1): QLinear(in_features=4096, out_features=14336, bias=False)
              (w2): QLinear(in_features=14336, out_features=4096, bias=False)
              (w3): QLinear(in_features=4096, out_features=14336, bias=False)
              (act_fn): SiLU()
            )
          )
        )
        (input_layernorm): MixtralRMSNorm()
        (post_attention_layernorm): MixtralRMSNorm()
      )
    )
    (norm): MixtralRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
)
wikitext2
Evaluating ...
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
Perplexity: 5.087006
Time:  261.8270502090454
Traceback (most recent call last):
  File "/home/vimagupta123/shivam/Mixture-Compressor-MoE/main.py", line 409, in <module>
    dataloader, testloader = get_loaders(
  File "/home/vimagupta123/shivam/Mixture-Compressor-MoE/datautils.py", line 107, in get_loaders
    loaders= get_c4(nsamples, seed, seqlen, model, tokenizer)
  File "/home/vimagupta123/shivam/Mixture-Compressor-MoE/datautils.py", line 64, in get_c4
    traindata = load_dataset('json', data_files={'train': 'data/c4-train.00000-of-01024.json'})
  File "/home/vimagupta123/shivam/Mixture-Compressor-MoE/mixture_compressor/lib/python3.10/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
  File "/home/vimagupta123/shivam/Mixture-Compressor-MoE/mixture_compressor/lib/python3.10/site-packages/datasets/load.py", line 1782, in load_dataset_builder
    dataset_module = dataset_module_factory(
  File "/home/vimagupta123/shivam/Mixture-Compressor-MoE/mixture_compressor/lib/python3.10/site-packages/datasets/load.py", line 1497, in dataset_module_factory
    ).get_module()
  File "/home/vimagupta123/shivam/Mixture-Compressor-MoE/mixture_compressor/lib/python3.10/site-packages/datasets/load.py", line 913, in get_module
    data_files = DataFilesDict.from_patterns(
  File "/home/vimagupta123/shivam/Mixture-Compressor-MoE/mixture_compressor/lib/python3.10/site-packages/datasets/data_files.py", line 690, in from_patterns
    else DataFilesList.from_patterns(
  File "/home/vimagupta123/shivam/Mixture-Compressor-MoE/mixture_compressor/lib/python3.10/site-packages/datasets/data_files.py", line 583, in from_patterns
    resolve_pattern(
  File "/home/vimagupta123/shivam/Mixture-Compressor-MoE/mixture_compressor/lib/python3.10/site-packages/datasets/data_files.py", line 384, in resolve_pattern
    raise FileNotFoundError(error_msg)
FileNotFoundError: Unable to find '/home/vimagupta123/shivam/Mixture-Compressor-MoE/data/c4-train.00000-of-01024.json'
