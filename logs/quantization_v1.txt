torch.int32                                                                                                                                                               [93/1859]
| block_sparse_moe.experts.4.w1-2 | 922907     | -          | -          | 3.794   |                                                                                               
torch.uint8                                                                                                                                                                        
| block_sparse_moe.experts.4.w2-2 | 504208.938 | -          | -          | 12.642  |                                                                                               
torch.uint8                                                                                                                                                                        
|                                 | 5          |            |            |         |                                                                                               
torch.uint8                                                                                                                                                                        
|                                 | 0          |            |            |         |                                                                                               
torch.uint8                                                                                                                                                                        
| block_sparse_moe.experts.5.w2-2 | 2602756    | -          | -          | 12.775  |                                                                                               
torch.uint8                                                                                                                                                                        
|                                 | 0          |            |            |         |                                                                                               
torch.uint8                                                                                                                                                                        
| block_sparse_moe.experts.6.w1-3 | 174213.406 | -          | -          | 4.247   |                                                                                               
torch.int32                                                                                                                                                                        
| block_sparse_moe.experts.6.w2-3 | 90448.836  | -          | -          | 13.601  |                                                                                               
torch.int32                                                                                                                                                                        
| block_sparse_moe.experts.6.w3-3 | 191907.031 | -          | -          | 4.311   |                                                                                               
torch.int32                                                                                                                                                                        
|                                 | 0          |            |            |         |                                                                                               
torch.uint8                                                                                                                                                                        
| block_sparse_moe.experts.7.w2-2 | 1125323    | -          | -          | 12.694  |                                                                                               
torch.uint8                                                                                                                                                                        
|                                 | 0          |            |            |         |                                                                                               
torch.uint8                                                                                                                                                                        
+--------------------------------+------------+------------+------------+---------+                                                                                                
                                                                                                                                                                                   
                                                                                                                                                                                   
quantization time: 6730.804841756821 s                                                                                                                                             
MixtralForCausalLM(                                                                                                                                                                
  (model): MixtralModel(  
    (embed_tokens): Embedding(32000, 4096)                                                                                                                                [62/1859]
    (layers): ModuleList(                                                                                                                                                          
      (0-31): 32 x MixtralDecoderLayer(                                                                                                                                            
        (self_attn): MixtralAttention(                                                                                                                                             
          (q_proj): QLinear(in_features=4096, out_features=4096, bias=False)                                                                                                       
          (k_proj): QLinear(in_features=4096, out_features=1024, bias=False)                                                                                                       
          (v_proj): QLinear(in_features=4096, out_features=1024, bias=False)                                                                                                       
          (o_proj): QLinear(in_features=4096, out_features=4096, bias=False)                                                                                                       
          (rotary_emb): MixtralRotaryEmbedding()                                                                                                                                   
        )                                                                                                                                                                          
        (block_sparse_moe): MixtralSparseMoeBlock(                                                                                                                                 
          (gate): QLinear(in_features=4096, out_features=8, bias=False)                                                                                                            
          (experts): ModuleList(                                                                                                                                                   
            (0-7): 8 x MixtralBLockSparseTop2MLP(                                                                                                                                  
              (w1): QLinear(in_features=4096, out_features=14336, bias=False)                                                                                                      
              (w2): QLinear(in_features=14336, out_features=4096, bias=False)                                                                                                      
              (w3): QLinear(in_features=4096, out_features=14336, bias=False)                                                                                                      
              (act_fn): SiLU()                                                                                                                                                     
            )                                                                                                                                                                      
          )                                                                                                                                                                        
        )                                                                                                                                                                          
        (input_layernorm): MixtralRMSNorm()                                                                                                                                        
        (post_attention_layernorm): MixtralRMSNorm()                                                                                                                               
      )                                                                                                                                                                            
    )                                                                                                                                                                              
    (norm): MixtralRMSNorm()                                                                                                                                                       
  )                                                                                                                                                                                
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)                                                                                                              
)                                                                                                                                                                                  
wikitext2                                                                                                                                                                          
Evaluating ... 
0                                                                                                                                                                         [31/1859]
1                                                                                                                                                                                  
2                                                                                                                                                                                  
3                                                                                                                                                                                  
4                                                                                                                                                                                  
5                                                                                                                                                                                  
6                                                                                                                                                                                  
7                                                                                                                                                                                  
8                                                                                                                                                                                  
9                                                                                                                                                                                  
10                                                                                                                                                                                 
11                                                                                                                                                                                 
12                                                                                                                                                                                 
13                                                                                                                                                                                 
14                                                                                                                                                                                 
15                                                                                                                                                                                 
16                                                                                                                                                                                 
17                                                                                                                                                                                 
18                                                                                                                                                                                 
19                                                                                                                                                                                 
20                                                                                                                                                                                 
21                                                                                                                                                                                 
22                                                                                                                                                                                 
23                                                                                                                                                                                 
24                                                                                                                                                                                 
25                                                                                                                                                                                 
26                                                                                                                                                                                 
27                                                                                                                                                                                 
28                                                                                                                                                                                 
29                                                                                                                                                                                 
30           
31                                                                                                                                                                         [0/1859]
Perplexity: 5.087006                        
Time:  260.60796999931335                   
Traceback (most recent call last):                                                       
  File "/home/vimagupta123/shivam/Mixture-Compressor-MoE/main.py", line 402, in <module>                                                                                           
    dataloader, testloader = get_loaders(                                                
  File "/home/vimagupta123/shivam/Mixture-Compressor-MoE/datautils.py", line 107, in get_loaders                                                                                   
    loaders= get_c4(nsamples, seed, seqlen, model, tokenizer)                            
  File "/home/vimagupta123/shivam/Mixture-Compressor-MoE/datautils.py", line 64, in get_c4                                                                                         
    traindata = load_dataset('json', data_files={'train': 'data/c4-train.00000-of-01024.json'})                                                                                    
  File "/home/vimagupta123/shivam/Mixture-Compressor-MoE/mixture_compressor/lib/python3.10/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(                                             
  File "/home/vimagupta123/shivam/Mixture-Compressor-MoE/mixture_compressor/lib/python3.10/site-packages/datasets/load.py", line 1782, in load_dataset_builder
    dataset_module = dataset_module_factory(                                             
  File "/home/vimagupta123/shivam/Mixture-Compressor-MoE/mixture_compressor/lib/python3.10/site-packages/datasets/load.py", line 1497, in dataset_module_factory
    ).get_module()                          
  File "/home/vimagupta123/shivam/Mixture-Compressor-MoE/mixture_compressor/lib/python3.10/site-packages/datasets/load.py", line 913, in get_module
    data_files = DataFilesDict.from_patterns(                                            
  File "/home/vimagupta123/shivam/Mixture-Compressor-MoE/mixture_compressor/lib/python3.10/site-packages/datasets/data_files.py", line 690, in from_patterns
    else DataFilesList.from_patterns(                                                    
  File "/home/vimagupta123/shivam/Mixture-Compressor-MoE/mixture_compressor/lib/python3.10/site-packages/datasets/data_files.py", line 583, in from_patterns
    resolve_pattern(                        
  File "/home/vimagupta123/shivam/Mixture-Compressor-MoE/mixture_compressor/lib/python3.10/site-packages/datasets/data_files.py", line 384, in resolve_pattern
    raise FileNotFoundError(error_msg)                                                   
FileNotFoundError: Unable to find '/home/vimagupta123/shivam/Mixture-Compressor-MoE/data/c4-train.00000-of-01024.json'     